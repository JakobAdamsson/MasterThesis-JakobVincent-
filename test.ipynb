{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from networks import *\n",
    "from data_handling import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torch import nn\n",
    "from skimage.filters import threshold_sauvola\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,jaccard_score\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a necessary transformation that has to be done on the images for the model to predict inputs\n",
    "patches_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def predict_input(pages:list, model:nn.Module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for i,page in enumerate(pages):\n",
    "        for j,patch in enumerate(page.grid): # All pages are divided into smaller patches, it is these patches that we actually predict they are then resynthesised into the larger image\n",
    "\n",
    "            patch_img = Image.fromarray(patch.img.astype('uint8'),'RGB')\n",
    "\n",
    "            transformed_image = patches_transforms(patch_img)\n",
    "            input_img = transformed_image.unsqueeze(0)\n",
    "            input_img= input_img.to(device)\n",
    "\n",
    "            with torch.no_grad(): # with no grad reduces runtime as no gradients are being monitored\n",
    "                output = model(input_img)['out'][0]\n",
    "            output_pred = output.argmax(0).byte()\n",
    "            page.grid[j].output = output_pred.cpu().numpy() #retrieves the prediction from device\n",
    "        pages[i].coarse_segmentation=page.reconstruct_prediction() # Recreates the image and stores it in the instance that was sent in.\n",
    "\n",
    "def evaluate_models(model:nn.Module, weights_path:str,testing_pages:list,thresholding_technique,name:str):\n",
    "    \"\"\"Evaluates a model with different weights on a set of pages.\n",
    "    Args:\n",
    "        model (module.nn): an empty model with no weights\n",
    "        weights_path (str): path to a directory full of weights for specified model\n",
    "        testing_pages (list): a list containing the pages class\n",
    "        thresholding_technique (lambda): a lambda function\n",
    "        name (string): used to name the outputted file\n",
    "\n",
    "    Returns:\n",
    "        dictionary: contains f1-score,precision,recall and IoU for each trial\n",
    "    \"\"\"\n",
    "    output = {'f1_score':[],'precision':[],'recall':[],'iou':[]}\n",
    "    model_weights = os.listdir(weights_path)\n",
    "\n",
    "    for weight in model_weights: # for each presaved weight test it on the pages\n",
    "        model.load_state_dict(torch.load(weights_path+\"/\"+weight))\n",
    "        predict_input(testing_pages,model)\n",
    "        preds_gt = [[],[]]\n",
    "\n",
    "        for i in range(len(testing_pages)):\n",
    "            refine_image(testing_pages[i],thresholding_technique)\n",
    "            preds_gt[0].append(testing_pages[i].refined_segmentation)\n",
    "            preds_gt[1].append(testing_pages[i].precise_gt)\n",
    "\n",
    "        pred_global = np.array(preds_gt[0]).flatten()\n",
    "        gt_global = np.array(preds_gt[1]).flatten()\n",
    "        # Uses scipy to calculate all the metrics sought after\n",
    "        output['f1_score'].append(f1_score(gt_global,pred_global,average=\"weighted\"))\n",
    "        output['recall'].append(recall_score(gt_global,pred_global,average=\"weighted\"))\n",
    "        output['precision'].append(precision_score(gt_global,pred_global,average=\"weighted\"))\n",
    "        output['iou'].append(jaccard_score(gt_global,pred_global,average=\"weighted\"))\n",
    "\n",
    "    pd.DataFrame(output).to_csv(f'{name}output.csv')\n",
    "    print(f\"metrics saved to '{name}output.csv'\")\n",
    "    return output\n",
    "\n",
    "def evaluate_models_by_manuscript(model:nn.Module, weights_path:str,num_pages:int,thresholding_technique,name:str):\n",
    "    \"\"\"Evaluates a model with different weights on the different manuscripts\n",
    "\n",
    "    Args:\n",
    "        model (module.nn): an empty model with no weights\n",
    "        weights_path (str): path to a directory full of weights for specified model\n",
    "        num_pages (int): number of pages per manuscript to evaluate on can be between 1-10, set to ten most of the time\n",
    "        thresholding_technique (lambda): a lambda function\n",
    "        name (string): used to name the outputted files\n",
    "    \"\"\"\n",
    "    manuscripts = {\"CS18\":{}, \"CS863\":{}, \"CB55\":{}}\n",
    "    for key in manuscripts.keys():\n",
    "        manuscripts[key] = {'f1_score':[],'precision':[],'recall':[],'iou':[]}\n",
    "\n",
    "    model_weights = os.listdir(weights_path) #should contain presaved weights for the model in question\n",
    "\n",
    "    for weight in model_weights: #for each pretraiend weight test it on these pages\n",
    "        model.load_state_dict(torch.load(weights_path+\"/\"+weight))\n",
    "        preds_gt = {}\n",
    "        for key in manuscripts.keys():\n",
    "            preds_gt[key] = [[],[]]\n",
    "            _,testing_pages,_ = generate_set(patch_size=224,resize=(1120, 1344),from_folders=(False,True,False),num_pages=num_pages,manuscripts=[key]) #returns a set of pages from the manuscript being evaluated\n",
    "            predict_input(testing_pages,model)\n",
    "            for i in range(len(testing_pages)):\n",
    "                refine_image(testing_pages[i],thresholding_technique)\n",
    "                preds_gt[key][0].append(testing_pages[i].refined_segmentation)\n",
    "                preds_gt[key][1].append(testing_pages[i].precise_gt)\n",
    "\n",
    "            pred_global = np.array(preds_gt[key][0]).flatten()\n",
    "            gt_global = np.array(preds_gt[key][1]).flatten()\n",
    "            manuscripts[key]['f1_score'].append(f1_score(gt_global,pred_global,average=\"weighted\"))\n",
    "            manuscripts[key]['recall'].append(recall_score(gt_global,pred_global,average=\"weighted\"))\n",
    "            manuscripts[key]['precision'].append(precision_score(gt_global,pred_global,average=\"weighted\"))\n",
    "            manuscripts[key]['iou'].append(jaccard_score(gt_global,pred_global,average=\"weighted\"))\n",
    "    for key in manuscripts.keys():\n",
    "        pd.DataFrame(manuscripts[key]).to_csv(f'{name}_output_{key}.csv')\n",
    "        print(f\"metrics saved to '{name}_output_{key}.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "The cell below contains the parameters which can be adjusted during the test, threshold_phan and sauv are the thresholding function and each of them have parameters which can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_set() got an unexpected keyword argument 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m threshold_sauv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m img: threshold_sauvola(img, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,k \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      3\u001b[0m num_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# any value between 1-10\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m _,testing_pages,_ \u001b[38;5;241m=\u001b[39m generate_set(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,resize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1120\u001b[39m, \u001b[38;5;241m1344\u001b[39m),from_folders\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28;01mFalse\u001b[39;00m),num_pages\u001b[38;5;241m=\u001b[39mnum_pages,filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Skola/Avancerad AI/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_set() got an unexpected keyword argument 'filepath'"
     ]
    }
   ],
   "source": [
    "threshold_phan = lambda img: phansalkar(img=img,n=15,p=0.3,q=3,k=0.15,R=0.5)\n",
    "threshold_sauv = lambda img: threshold_sauvola(img, window_size=15,k =0.05)\n",
    "num_pages = 10 # any value between 1-10\n",
    "_,testing_pages,_ = generate_set(patch_size=224,resize=(1120, 1344),from_folders=(False,True,False),num_pages=num_pages,filepath=\"D:/Skola/Avancerad AI/\")\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "metrics saved to 'UNet_phan_output_CS18.csv'\n",
      "metrics saved to 'UNet_phan_output_CS863.csv'\n",
      "metrics saved to 'UNet_phan_output_CB55.csv'\n",
      "metrics saved to 'UNet_sauv_output_CS18.csv'\n",
      "metrics saved to 'UNet_sauv_output_CS863.csv'\n",
      "metrics saved to 'UNet_sauv_output_CB55.csv'\n",
      "metrics saved to 'DeepLabV3_phan_output_CS18.csv'\n",
      "metrics saved to 'DeepLabV3_phan_output_CS863.csv'\n",
      "metrics saved to 'DeepLabV3_phan_output_CB55.csv'\n",
      "metrics saved to 'DeepLabV3_sauv_output_CS18.csv'\n",
      "metrics saved to 'DeepLabV3_sauv_output_CS863.csv'\n",
      "metrics saved to 'DeepLabV3_sauv_output_CB55.csv'\n"
     ]
    }
   ],
   "source": [
    "#Saves all the metrics retrieved in several csv files. This is tedious but we could not find a neat solution\n",
    "model = UNet(n_class=4)\n",
    "model_weights_file_path = \"D:\\Hämtade Filer\\models-20240320T180531Z-001\\models\"\n",
    "evaluate_models_by_manuscript(model,model_weights_file_path,num_pages,threshold_phan,name=\"UNet_phan\")\n",
    "evaluate_models_by_manuscript(model,model_weights_file_path,num_pages,threshold_sauv,name=\"UNet_sauv\")\n",
    "\n",
    "model = deeplabv3_resnet50(num_classes=4)\n",
    "model_weights_file_path = \"modelsDeeplab\"\n",
    "evaluate_models_by_manuscript(model,model_weights_file_path,num_pages,threshold_phan,name=\"DeepLabV3_phan\")\n",
    "evaluate_models_by_manuscript(model,model_weights_file_path,num_pages,threshold_sauv,name=\"DeepLabV3_sauv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis is done on the overall manuscript for the sake of brevity and to fit within the time constraint, it would be interesting to examine each dataset (manuscript) by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics saved to 'DeepLabV3_phanoutput.csv'\n",
      "metrics saved to 'DeepLabV3_sauvoutput.csv'\n",
      "metrics saved to 'UNet_phanoutput.csv'\n",
      "metrics saved to 'UNet_sauvoutput.csv'\n"
     ]
    }
   ],
   "source": [
    "model = deeplabv3_resnet50(num_classes=4)\n",
    "model_weights_file_path = \"modelsDeeplab\"\n",
    "outputs['DP']=evaluate_models(model,model_weights_file_path,testing_pages,threshold_phan,name=\"DeepLabV3_phan\")\n",
    "outputs['DS']=evaluate_models(model,model_weights_file_path,testing_pages,threshold_sauv,name=\"DeepLabV3_sauv\")\n",
    "\n",
    "model = UNet(n_class=4)\n",
    "model_weights_file_path = \"D:\\Hämtade Filer\\models-20240320T180531Z-001\\models\"\n",
    "outputs['UP']=evaluate_models(model,model_weights_file_path,testing_pages,threshold_phan,name=\"UNet_phan\")\n",
    "outputs['US']=evaluate_models(model,model_weights_file_path,testing_pages,threshold_sauv,name=\"UNet_sauv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to fit within the perview of the course, only f1-scores are of concern, they're extracted out of the output dictionary to conduct the friedman tests and nemenyi later on if differences are found. It would have been fun to use the friedman test that was done in the machine learning course instead of a prebuilt one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "for key in outputs.keys():\n",
    "    f1_scores[key] = outputs[key]['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores= {    'DP': [0.9751, 0.9763, 0.9761, 0.9707, 0.9756, 0.9764, 0.9753, 0.9760, 0.9753, 0.9757],\n",
    "    'DS': [0.9805, 0.9819, 0.9814, 0.9755, 0.9811, 0.9819, 0.9806, 0.9816, 0.9804, 0.9812],\n",
    "    'UP': [0.9703, 0.9513, 0.9601, 0.9738, 0.9617, 0.9748, 0.9653, 0.9496, 0.9607, 0.9630],\n",
    "    'US': [0.9752, 0.9560, 0.9647, 0.9787, 0.9663, 0.9798, 0.9703, 0.9534, 0.9653, 0.9677]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FriedmanchisquareResult(statistic=23.639999999999986, pvalue=2.9698131231310194e-05)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "friedmanchisquare(f1_scores['DP'],f1_scores['DS'],f1_scores['UP'],f1_scores['US'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a p-value of 2.96 is below the confidence interval of 0.05. Furthermore the friedman statistic is far above the require 7.68 for four populations with n = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DP</th>\n",
       "      <th>DS</th>\n",
       "      <th>UP</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DP</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109694</td>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS</th>\n",
       "      <td>0.109694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.046280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UP</th>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.109694</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DP        DS        UP        US\n",
       "DP  1.000000  0.109694  0.046280  0.900000\n",
       "DS  0.109694  1.000000  0.001000  0.046280\n",
       "UP  0.046280  0.001000  1.000000  0.109694\n",
       "US  0.900000  0.046280  0.109694  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import scikit_posthocs as sp\n",
    "data = []\n",
    "names = {2:\"UP\",3:\"US\",0:\"DP\",1:\"DS\"} # Some renaming to make the table a bit more understandable :)\n",
    "for key in f1_scores.keys():\n",
    "    data.append(f1_scores[key])\n",
    "data= np.array(data)\n",
    "nemenyi = sp.posthoc_nemenyi_friedman(data.T)\n",
    "nemenyi.rename(names,inplace=True)\n",
    "nemenyi.rename(names,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DP vs DS: not significant\n",
    "* DP vs UP: significant\n",
    "* DP vs US: not significant\n",
    "* DS vs UP: significant\n",
    "* DS vs US: significant\n",
    "* UP vs US: not significant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
