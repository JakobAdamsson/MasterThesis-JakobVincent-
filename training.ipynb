{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handling import *\n",
    "from networks import *\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class model_type(Enum):\n",
    "    U_Net = 1,\n",
    "    DeepLabV3 = 2,\n",
    "    L_U_Net = 3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "patches_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "def load_model(model:model_type,num_classes=4):\n",
    "    if model == model_type.U_Net:\n",
    "        return UNet(num_classes)\n",
    "    elif model == model_type.DeepLabV3:\n",
    "        return deeplabv3_resnet50(num_classes=num_classes)\n",
    "    elif model == model_type.L_U_Net:\n",
    "        return L_U_Net(num_classes,16)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, delta=0,file_name:str=\"checkpoint.pth\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs should we wait after last time validation loss improved.\n",
    "                            Default: 20\n",
    "            delta (float): Minimum change in the loss to qualify as an improvement.\n",
    "                           Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.early_stop = False\n",
    "        self.name = file_name\n",
    "\n",
    "    def __call__(self, val_loss, model,verbose=False):\n",
    "\n",
    "        if (val_loss + self.delta < self.val_loss_min):\n",
    "            self.save_checkpoint(val_loss, model,verbose)\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model,verbose):\n",
    "        if(verbose):\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.name)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_subset,val_subset,_ \u001b[38;5;241m=\u001b[39m generate_set(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,resize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1120\u001b[39m, \u001b[38;5;241m1344\u001b[39m),filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Skola/Avancerad AI/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Done to created weights for the loss function, it might be better to do this on patches to include the true distribution based on dynamically added crops\u001b[39;00m\n\u001b[0;32m     20\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_classes)\n",
      "File \u001b[1;32md:\\Skola\\DeepLearning Project\\data_handling.py:283\u001b[0m, in \u001b[0;36mgenerate_set\u001b[1;34m(num_pages, manuscripts, patch_size, resize, from_folders, filepath)\u001b[0m\n\u001b[0;32m    274\u001b[0m     data_dict[manuscript] \u001b[38;5;241m=\u001b[39m parse_images_from_folder(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/img-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/img/training\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/PAGE-gt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/PAGE-gt/training\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    280\u001b[0m         resize\u001b[38;5;241m=\u001b[39mresize,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_folders[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 283\u001b[0m     val_dict[manuscript] \u001b[38;5;241m=\u001b[39m parse_images_from_folder(\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/img-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/img/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/PAGE-gt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/PAGE-gt/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/pixel-level-gt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pixel-level-gt/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    287\u001b[0m         num_pages,\n\u001b[0;32m    288\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39mpatch_size,\n\u001b[0;32m    289\u001b[0m         resize\u001b[38;5;241m=\u001b[39mresize,\n\u001b[0;32m    290\u001b[0m     )\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_folders[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m    292\u001b[0m     test_dict[manuscript] \u001b[38;5;241m=\u001b[39m parse_images_from_folder(\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/img-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/img/public-test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all/PAGE-gt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanuscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/PAGE-gt/public-test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m         resize\u001b[38;5;241m=\u001b[39mresize,\n\u001b[0;32m    299\u001b[0m     )\n",
      "File \u001b[1;32md:\\Skola\\DeepLearning Project\\data_handling.py:236\u001b[0m, in \u001b[0;36mparse_images_from_folder\u001b[1;34m(image_dir_path, xml_dir_path, pixel_dir_path, num_pages, patch_size, precise, resize)\u001b[0m\n\u001b[0;32m    234\u001b[0m     xml_file_path \u001b[38;5;241m=\u001b[39m xml_dir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m image_files[i][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m     pixel_gt_file_path \u001b[38;5;241m=\u001b[39m pixel_dir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m image_files[i][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 236\u001b[0m     page \u001b[38;5;241m=\u001b[39m Page\u001b[38;5;241m.\u001b[39mfrom_file(\n\u001b[0;32m    237\u001b[0m         img_file_path,\n\u001b[0;32m    238\u001b[0m         xml_file_path,\n\u001b[0;32m    239\u001b[0m         pixel_gt_file_path,\n\u001b[0;32m    240\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39mpatch_size,\n\u001b[0;32m    241\u001b[0m         resize\u001b[38;5;241m=\u001b[39mresize,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[0;32m    243\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(page)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\Skola\\DeepLearning Project\\data_handling.py:154\u001b[0m, in \u001b[0;36mPage.from_file\u001b[1;34m(self, image_path, xml_path, pixel_gt_path, patch_size, resize)\u001b[0m\n\u001b[0;32m    152\u001b[0m gt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(filled_polygon_img)\n\u001b[0;32m    153\u001b[0m gt_precise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(gt)\n\u001b[1;32m--> 154\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pixel_gt)\n\u001b[0;32m    155\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[:, :, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# All information is stored in the red colour channel.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Calculates the pixel precise  ground truth by taking the non zero intersection with pixel precise annotations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dator\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32mc:\\Users\\dator\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m    756\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m--> 758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dator\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 4\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-4\n",
    "decay = 1e-5\n",
    "num_trials = 10\n",
    "num_classes = 4\n",
    "losses = [[[] for _ in range(num_trials)],[[] for _ in range(num_trials)]] # training , validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for i in range(num_trials):\n",
    "\n",
    "    #Change the enum to U_Net to train a U-net model.\n",
    "    model = load_model(model_type.L_U_Net,num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Get data\n",
    "    train_subset,val_subset,_ = generate_set(patch_size=224,resize=(1120, 1344),filepath=\"D:/Skola/Avancerad AI/\",indices=i*2)\n",
    "\n",
    "    # Done to created weights for the loss function, it might be better to do this on patches to include the true distribution based on dynamically added crops\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    for page in train_subset: #Sum up the number of pixels for each class for each page\n",
    "        for class_id in range(num_classes):\n",
    "            class_counts[class_id] += np.sum(page.gt == class_id)\n",
    "\n",
    "    total_pixels = np.sum(class_counts) #Count the total of pixels\n",
    "    frequencies = class_counts / total_pixels\n",
    "    weights = torch.tensor(np.sqrt(1.0 / frequencies).astype('float32')) # calculate weights\n",
    "    weights\n",
    "    weights = weights.to(device)\n",
    "    # Loss, optimizer, stopper\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=decay)\n",
    "    earlystopper= EarlyStopping(20,0, f'./model-small-fold-{i}.pth')\n",
    "\n",
    "\n",
    "    #Prepare data\n",
    "    train_dataset = PatchesDataset(train_subset,patches_transforms)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = PatchesDataset(val_subset,patches_transforms)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_dataset.random_patch_generator(10)\n",
    "        val_dataset.random_patch_generator(10)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        losses[0].append(train_loss)\n",
    "\n",
    "        #Validation loop\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)['out']\n",
    "                loss = criterion(outputs,labels)\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "        val_loss = running_loss/len(val_loader.dataset)\n",
    "        losses[1][i].append(val_loss)\n",
    "        if epoch > 50:\n",
    "            earlystopper(val_loss=val_loss,model=model,verbose=False)\n",
    "        if earlystopper.early_stop == True:\n",
    "                break\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train loss: {loss.item():.4f}, Validation loss {val_loss:.4f}')\n",
    "\n",
    "    print(f'Training finished! after {epoch} epochs, Model saved to ./model-fold-{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train loss: 0.9604, Validation loss 1.0720\n",
      "Epoch [2/200], Train loss: 0.9667, Validation loss 1.0302\n",
      "Epoch [3/200], Train loss: 1.0803, Validation loss 0.9731\n",
      "Epoch [4/200], Train loss: 0.9255, Validation loss 0.8948\n",
      "Epoch [5/200], Train loss: 0.8964, Validation loss 0.8515\n",
      "Epoch [6/200], Train loss: 0.7776, Validation loss 0.8097\n",
      "Epoch [7/200], Train loss: 0.6740, Validation loss 0.7065\n",
      "Epoch [8/200], Train loss: 0.7098, Validation loss 0.6804\n",
      "Epoch [9/200], Train loss: 0.8940, Validation loss 0.6806\n",
      "Epoch [10/200], Train loss: 0.6275, Validation loss 0.5988\n",
      "Epoch [11/200], Train loss: 0.8555, Validation loss 0.6652\n",
      "Epoch [12/200], Train loss: 0.8095, Validation loss 0.5902\n",
      "Epoch [13/200], Train loss: 0.5464, Validation loss 0.5674\n",
      "Epoch [14/200], Train loss: 0.4631, Validation loss 0.6138\n",
      "Epoch [15/200], Train loss: 0.5176, Validation loss 0.5461\n",
      "Epoch [16/200], Train loss: 0.7484, Validation loss 0.4826\n",
      "Epoch [17/200], Train loss: 0.4501, Validation loss 0.4877\n",
      "Epoch [18/200], Train loss: 0.3653, Validation loss 0.4136\n",
      "Epoch [19/200], Train loss: 0.3475, Validation loss 0.4567\n",
      "Epoch [20/200], Train loss: 0.4108, Validation loss 0.3931\n",
      "Epoch [21/200], Train loss: 0.2636, Validation loss 0.3886\n",
      "Epoch [22/200], Train loss: 0.4004, Validation loss 0.3849\n",
      "Epoch [23/200], Train loss: 0.4205, Validation loss 0.4000\n",
      "Epoch [24/200], Train loss: 0.3008, Validation loss 0.3472\n",
      "Epoch [25/200], Train loss: 0.2880, Validation loss 0.3316\n",
      "Epoch [26/200], Train loss: 0.3286, Validation loss 0.3208\n",
      "Epoch [27/200], Train loss: 0.3251, Validation loss 0.3348\n",
      "Epoch [28/200], Train loss: 0.4081, Validation loss 0.3103\n",
      "Epoch [29/200], Train loss: 0.3298, Validation loss 0.2963\n",
      "Epoch [30/200], Train loss: 0.3000, Validation loss 0.2789\n",
      "Epoch [31/200], Train loss: 0.3303, Validation loss 0.3274\n",
      "Epoch [32/200], Train loss: 0.3123, Validation loss 0.2925\n",
      "Epoch [33/200], Train loss: 0.2919, Validation loss 0.2936\n",
      "Epoch [34/200], Train loss: 0.3480, Validation loss 0.2652\n",
      "Epoch [35/200], Train loss: 0.3095, Validation loss 0.2683\n",
      "Epoch [36/200], Train loss: 0.2790, Validation loss 0.2585\n",
      "Epoch [37/200], Train loss: 0.1653, Validation loss 0.2749\n",
      "Epoch [38/200], Train loss: 0.3478, Validation loss 0.3822\n",
      "Epoch [39/200], Train loss: 0.3630, Validation loss 0.2543\n",
      "Epoch [40/200], Train loss: 0.2475, Validation loss 0.2804\n",
      "Epoch [41/200], Train loss: 0.2860, Validation loss 0.2636\n",
      "Epoch [42/200], Train loss: 0.1838, Validation loss 0.2374\n",
      "Epoch [43/200], Train loss: 0.2456, Validation loss 0.2479\n",
      "Epoch [44/200], Train loss: 0.2718, Validation loss 0.2234\n",
      "Epoch [45/200], Train loss: 0.2399, Validation loss 0.2331\n",
      "Epoch [46/200], Train loss: 0.1976, Validation loss 0.2274\n",
      "Epoch [47/200], Train loss: 0.2347, Validation loss 0.2385\n",
      "Epoch [48/200], Train loss: 0.1458, Validation loss 0.2120\n",
      "Epoch [49/200], Train loss: 0.2450, Validation loss 0.2127\n",
      "Epoch [50/200], Train loss: 0.2955, Validation loss 0.2298\n",
      "Epoch [51/200], Train loss: 0.2580, Validation loss 0.2272\n",
      "Epoch [52/200], Train loss: 0.1927, Validation loss 0.2386\n",
      "Epoch [53/200], Train loss: 0.2387, Validation loss 0.2852\n",
      "Epoch [54/200], Train loss: 0.1271, Validation loss 0.2311\n",
      "Epoch [55/200], Train loss: 0.1525, Validation loss 0.2432\n",
      "Epoch [56/200], Train loss: 0.3530, Validation loss 0.2983\n",
      "Epoch [57/200], Train loss: 0.2329, Validation loss 0.2133\n",
      "Epoch [58/200], Train loss: 0.2347, Validation loss 0.2090\n",
      "Epoch [59/200], Train loss: 0.2082, Validation loss 0.2089\n",
      "Epoch [60/200], Train loss: 0.1654, Validation loss 0.2049\n",
      "Epoch [61/200], Train loss: 0.3080, Validation loss 0.2034\n",
      "Epoch [62/200], Train loss: 0.1518, Validation loss 0.2278\n",
      "Epoch [63/200], Train loss: 0.4044, Validation loss 0.3029\n",
      "Epoch [64/200], Train loss: 0.1524, Validation loss 0.2030\n",
      "Epoch [65/200], Train loss: 0.2032, Validation loss 0.2067\n",
      "Epoch [66/200], Train loss: 0.3692, Validation loss 0.2559\n",
      "Epoch [67/200], Train loss: 0.2177, Validation loss 0.2446\n",
      "Epoch [68/200], Train loss: 0.2499, Validation loss 0.2222\n",
      "Epoch [69/200], Train loss: 0.0319, Validation loss 0.1829\n",
      "Epoch [70/200], Train loss: 0.2750, Validation loss 0.1998\n",
      "Epoch [71/200], Train loss: 0.2556, Validation loss 0.2078\n",
      "Epoch [72/200], Train loss: 0.2792, Validation loss 0.1909\n",
      "Epoch [73/200], Train loss: 0.1296, Validation loss 0.2098\n",
      "Epoch [74/200], Train loss: 0.1424, Validation loss 0.2190\n",
      "Epoch [75/200], Train loss: 0.1566, Validation loss 0.2288\n",
      "Epoch [76/200], Train loss: 0.2942, Validation loss 0.2161\n",
      "Epoch [77/200], Train loss: 0.1749, Validation loss 0.2241\n",
      "Epoch [78/200], Train loss: 0.2915, Validation loss 0.2173\n",
      "Epoch [79/200], Train loss: 0.1506, Validation loss 0.2269\n",
      "Epoch [80/200], Train loss: 0.1749, Validation loss 0.1949\n",
      "Epoch [81/200], Train loss: 0.2578, Validation loss 0.1966\n",
      "Epoch [82/200], Train loss: 0.1156, Validation loss 0.2042\n",
      "Epoch [83/200], Train loss: 0.2389, Validation loss 0.2327\n",
      "Epoch [84/200], Train loss: 0.1666, Validation loss 0.2224\n",
      "Epoch [85/200], Train loss: 0.1621, Validation loss 0.2225\n",
      "Epoch [86/200], Train loss: 0.1284, Validation loss 0.1998\n",
      "Epoch [87/200], Train loss: 0.2165, Validation loss 0.1921\n",
      "Epoch [88/200], Train loss: 0.3309, Validation loss 0.2289\n",
      "Training finished! after 88 epochs, Model saved to ./model-fold-0.pth\n",
      "Epoch [1/200], Train loss: 1.2009, Validation loss 1.0983\n",
      "Epoch [2/200], Train loss: 1.0364, Validation loss 1.0217\n",
      "Epoch [3/200], Train loss: 0.8822, Validation loss 0.9239\n",
      "Epoch [4/200], Train loss: 0.9489, Validation loss 0.8841\n",
      "Epoch [5/200], Train loss: 0.7833, Validation loss 0.8271\n",
      "Epoch [6/200], Train loss: 0.7914, Validation loss 0.7737\n",
      "Epoch [7/200], Train loss: 0.5751, Validation loss 0.7533\n",
      "Epoch [8/200], Train loss: 0.8643, Validation loss 0.6865\n",
      "Epoch [9/200], Train loss: 0.8065, Validation loss 0.6987\n",
      "Epoch [10/200], Train loss: 0.7775, Validation loss 0.6612\n",
      "Epoch [11/200], Train loss: 0.5688, Validation loss 0.6532\n",
      "Epoch [12/200], Train loss: 0.6359, Validation loss 0.6189\n",
      "Epoch [13/200], Train loss: 0.7809, Validation loss 0.6289\n",
      "Epoch [14/200], Train loss: 0.5377, Validation loss 0.4743\n",
      "Epoch [15/200], Train loss: 0.5707, Validation loss 0.5068\n",
      "Epoch [16/200], Train loss: 0.4899, Validation loss 0.4811\n",
      "Epoch [17/200], Train loss: 0.5344, Validation loss 0.4737\n",
      "Epoch [18/200], Train loss: 0.3946, Validation loss 0.5038\n",
      "Epoch [19/200], Train loss: 0.5683, Validation loss 0.4133\n",
      "Epoch [20/200], Train loss: 0.3857, Validation loss 0.4075\n",
      "Epoch [21/200], Train loss: 0.4008, Validation loss 0.4784\n",
      "Epoch [22/200], Train loss: 0.3615, Validation loss 0.3925\n",
      "Epoch [23/200], Train loss: 0.3187, Validation loss 0.3932\n",
      "Epoch [24/200], Train loss: 0.4769, Validation loss 0.4168\n",
      "Epoch [25/200], Train loss: 0.3732, Validation loss 0.4077\n",
      "Epoch [26/200], Train loss: 0.3085, Validation loss 0.3279\n",
      "Epoch [27/200], Train loss: 0.3751, Validation loss 0.3808\n",
      "Epoch [28/200], Train loss: 0.3273, Validation loss 0.3287\n",
      "Epoch [29/200], Train loss: 0.3620, Validation loss 0.3491\n",
      "Epoch [30/200], Train loss: 0.3543, Validation loss 0.3681\n",
      "Epoch [31/200], Train loss: 0.3742, Validation loss 0.3579\n",
      "Epoch [32/200], Train loss: 0.2664, Validation loss 0.4604\n",
      "Epoch [33/200], Train loss: 0.2506, Validation loss 0.2955\n",
      "Epoch [34/200], Train loss: 0.3933, Validation loss 0.2917\n",
      "Epoch [35/200], Train loss: 0.2550, Validation loss 0.2737\n",
      "Epoch [36/200], Train loss: 0.3258, Validation loss 0.2979\n",
      "Epoch [37/200], Train loss: 0.2416, Validation loss 0.2914\n",
      "Epoch [38/200], Train loss: 0.1718, Validation loss 0.2666\n",
      "Epoch [39/200], Train loss: 0.2489, Validation loss 0.2559\n",
      "Epoch [40/200], Train loss: 0.1680, Validation loss 0.2438\n",
      "Epoch [41/200], Train loss: 0.4506, Validation loss 0.2650\n",
      "Epoch [42/200], Train loss: 0.3258, Validation loss 0.3056\n",
      "Epoch [43/200], Train loss: 0.3811, Validation loss 0.2421\n",
      "Epoch [44/200], Train loss: 0.2601, Validation loss 0.3099\n",
      "Epoch [45/200], Train loss: 0.3043, Validation loss 0.2517\n",
      "Epoch [46/200], Train loss: 0.3188, Validation loss 0.3062\n",
      "Epoch [47/200], Train loss: 0.0968, Validation loss 0.3068\n",
      "Epoch [48/200], Train loss: 0.3669, Validation loss 0.3949\n",
      "Epoch [49/200], Train loss: 0.2671, Validation loss 0.2528\n",
      "Epoch [50/200], Train loss: 0.2731, Validation loss 0.2438\n",
      "Epoch [51/200], Train loss: 0.1916, Validation loss 0.2904\n",
      "Epoch [52/200], Train loss: 0.2600, Validation loss 0.2638\n",
      "Epoch [53/200], Train loss: 0.3440, Validation loss 0.2378\n",
      "Epoch [54/200], Train loss: 0.1671, Validation loss 0.2424\n",
      "Epoch [55/200], Train loss: 0.1335, Validation loss 0.2345\n",
      "Epoch [56/200], Train loss: 0.1434, Validation loss 0.2795\n",
      "Epoch [57/200], Train loss: 0.2168, Validation loss 0.3749\n",
      "Epoch [58/200], Train loss: 0.2955, Validation loss 0.2763\n",
      "Epoch [59/200], Train loss: 0.4603, Validation loss 0.2522\n",
      "Epoch [60/200], Train loss: 0.4515, Validation loss 0.6729\n",
      "Epoch [61/200], Train loss: 0.7233, Validation loss 0.3685\n",
      "Epoch [62/200], Train loss: 0.3032, Validation loss 0.2100\n",
      "Epoch [63/200], Train loss: 0.2891, Validation loss 0.1996\n",
      "Epoch [64/200], Train loss: 0.1370, Validation loss 0.2195\n",
      "Epoch [65/200], Train loss: 0.1950, Validation loss 0.2271\n",
      "Epoch [66/200], Train loss: 0.1699, Validation loss 0.2129\n",
      "Epoch [67/200], Train loss: 0.1872, Validation loss 0.1853\n",
      "Epoch [68/200], Train loss: 0.2629, Validation loss 0.4042\n",
      "Epoch [69/200], Train loss: 0.2605, Validation loss 0.3391\n",
      "Epoch [70/200], Train loss: 0.4998, Validation loss 0.3006\n",
      "Epoch [71/200], Train loss: 1.3917, Validation loss 0.9327\n",
      "Epoch [72/200], Train loss: 0.1594, Validation loss 0.2129\n",
      "Epoch [73/200], Train loss: 0.2054, Validation loss 0.2679\n",
      "Epoch [74/200], Train loss: 0.2694, Validation loss 0.2903\n",
      "Epoch [75/200], Train loss: 0.7088, Validation loss 0.4273\n",
      "Epoch [76/200], Train loss: 0.3143, Validation loss 0.2680\n",
      "Epoch [77/200], Train loss: 0.2073, Validation loss 0.2260\n",
      "Epoch [78/200], Train loss: 0.2966, Validation loss 0.2066\n",
      "Epoch [79/200], Train loss: 0.2247, Validation loss 0.1996\n",
      "Epoch [80/200], Train loss: 0.2259, Validation loss 0.2395\n",
      "Epoch [81/200], Train loss: 0.3962, Validation loss 0.2616\n",
      "Epoch [82/200], Train loss: 0.2542, Validation loss 0.2290\n",
      "Epoch [83/200], Train loss: 0.0903, Validation loss 0.2001\n",
      "Epoch [84/200], Train loss: 0.4248, Validation loss 0.3528\n",
      "Epoch [85/200], Train loss: 0.0325, Validation loss 0.3258\n",
      "Epoch [86/200], Train loss: 0.2846, Validation loss 0.2558\n",
      "Training finished! after 86 epochs, Model saved to ./model-fold-1.pth\n",
      "Epoch [1/200], Train loss: 1.2348, Validation loss 1.1961\n",
      "Epoch [2/200], Train loss: 1.2530, Validation loss 1.1054\n",
      "Epoch [3/200], Train loss: 1.0897, Validation loss 1.0399\n",
      "Epoch [4/200], Train loss: 1.0038, Validation loss 0.9762\n",
      "Epoch [5/200], Train loss: 0.8309, Validation loss 0.8739\n",
      "Epoch [6/200], Train loss: 0.8371, Validation loss 0.8248\n",
      "Epoch [7/200], Train loss: 0.7992, Validation loss 0.7708\n",
      "Epoch [8/200], Train loss: 0.6333, Validation loss 0.7309\n",
      "Epoch [9/200], Train loss: 0.7718, Validation loss 0.6979\n",
      "Epoch [10/200], Train loss: 0.8131, Validation loss 0.7053\n",
      "Epoch [11/200], Train loss: 0.5265, Validation loss 0.6415\n",
      "Epoch [12/200], Train loss: 0.7026, Validation loss 0.6241\n",
      "Epoch [13/200], Train loss: 0.5514, Validation loss 0.5849\n",
      "Epoch [14/200], Train loss: 0.5451, Validation loss 0.5613\n",
      "Epoch [15/200], Train loss: 0.5289, Validation loss 0.5561\n",
      "Epoch [16/200], Train loss: 0.5000, Validation loss 0.5523\n",
      "Epoch [17/200], Train loss: 0.4947, Validation loss 0.5169\n",
      "Epoch [18/200], Train loss: 0.5216, Validation loss 0.5134\n",
      "Epoch [19/200], Train loss: 0.4944, Validation loss 0.4588\n",
      "Epoch [20/200], Train loss: 0.5110, Validation loss 0.4539\n",
      "Epoch [21/200], Train loss: 0.3448, Validation loss 0.4152\n",
      "Epoch [22/200], Train loss: 0.3243, Validation loss 0.3970\n",
      "Epoch [23/200], Train loss: 0.4505, Validation loss 0.4023\n",
      "Epoch [24/200], Train loss: 0.3127, Validation loss 0.4103\n",
      "Epoch [25/200], Train loss: 0.5072, Validation loss 0.3718\n",
      "Epoch [26/200], Train loss: 0.3511, Validation loss 0.3804\n",
      "Epoch [27/200], Train loss: 0.3494, Validation loss 0.3421\n",
      "Epoch [28/200], Train loss: 0.3630, Validation loss 0.3347\n",
      "Epoch [29/200], Train loss: 0.2411, Validation loss 0.3211\n",
      "Epoch [30/200], Train loss: 0.2669, Validation loss 0.3262\n",
      "Epoch [31/200], Train loss: 0.3117, Validation loss 0.3079\n",
      "Epoch [32/200], Train loss: 0.2605, Validation loss 0.3149\n",
      "Epoch [33/200], Train loss: 0.2205, Validation loss 0.3036\n",
      "Epoch [34/200], Train loss: 0.2910, Validation loss 0.2834\n",
      "Epoch [35/200], Train loss: 0.3426, Validation loss 0.2696\n",
      "Epoch [36/200], Train loss: 0.3040, Validation loss 0.2805\n",
      "Epoch [37/200], Train loss: 0.2958, Validation loss 0.2811\n",
      "Epoch [38/200], Train loss: 0.2926, Validation loss 0.2628\n",
      "Epoch [39/200], Train loss: 0.2029, Validation loss 0.2434\n",
      "Epoch [40/200], Train loss: 0.2334, Validation loss 0.2506\n",
      "Epoch [41/200], Train loss: 0.1934, Validation loss 0.2320\n",
      "Epoch [42/200], Train loss: 0.2076, Validation loss 0.2506\n",
      "Epoch [43/200], Train loss: 0.2129, Validation loss 0.2349\n",
      "Epoch [44/200], Train loss: 0.2207, Validation loss 0.2308\n",
      "Epoch [45/200], Train loss: 0.1500, Validation loss 0.2330\n",
      "Epoch [46/200], Train loss: 0.2671, Validation loss 0.2130\n",
      "Epoch [47/200], Train loss: 0.1832, Validation loss 0.2100\n",
      "Epoch [48/200], Train loss: 0.1429, Validation loss 0.2085\n",
      "Epoch [49/200], Train loss: 0.3688, Validation loss 0.2195\n",
      "Epoch [50/200], Train loss: 0.2209, Validation loss 0.2056\n",
      "Epoch [51/200], Train loss: 0.2319, Validation loss 0.2126\n",
      "Epoch [52/200], Train loss: 0.1803, Validation loss 0.2144\n",
      "Epoch [53/200], Train loss: 0.1047, Validation loss 0.2222\n",
      "Epoch [54/200], Train loss: 0.2067, Validation loss 0.2023\n",
      "Epoch [55/200], Train loss: 0.1401, Validation loss 0.1952\n",
      "Epoch [56/200], Train loss: 0.2214, Validation loss 0.2057\n",
      "Epoch [57/200], Train loss: 0.3503, Validation loss 0.2045\n",
      "Epoch [58/200], Train loss: 0.2528, Validation loss 0.1818\n",
      "Epoch [59/200], Train loss: 0.2600, Validation loss 0.2095\n",
      "Epoch [60/200], Train loss: 0.2282, Validation loss 0.1849\n",
      "Epoch [61/200], Train loss: 0.2615, Validation loss 0.1861\n",
      "Epoch [62/200], Train loss: 0.1596, Validation loss 0.1966\n",
      "Epoch [63/200], Train loss: 0.2379, Validation loss 0.1895\n",
      "Epoch [64/200], Train loss: 0.1844, Validation loss 0.2046\n",
      "Epoch [65/200], Train loss: 0.2057, Validation loss 0.1779\n",
      "Epoch [66/200], Train loss: 0.1034, Validation loss 0.2387\n",
      "Epoch [67/200], Train loss: 0.1741, Validation loss 0.1742\n",
      "Epoch [68/200], Train loss: 0.2690, Validation loss 0.1700\n",
      "Epoch [69/200], Train loss: 0.2141, Validation loss 0.1726\n",
      "Epoch [70/200], Train loss: 0.2185, Validation loss 0.1864\n",
      "Epoch [71/200], Train loss: 0.2507, Validation loss 0.1852\n",
      "Epoch [72/200], Train loss: 0.2151, Validation loss 0.1709\n",
      "Epoch [73/200], Train loss: 0.1633, Validation loss 0.1750\n",
      "Epoch [74/200], Train loss: 0.0861, Validation loss 0.1717\n",
      "Epoch [75/200], Train loss: 0.1444, Validation loss 0.1840\n",
      "Epoch [76/200], Train loss: 0.2337, Validation loss 0.1679\n",
      "Epoch [77/200], Train loss: 0.0412, Validation loss 0.1796\n",
      "Epoch [78/200], Train loss: 0.1349, Validation loss 0.1644\n",
      "Epoch [79/200], Train loss: 0.1899, Validation loss 0.1659\n",
      "Epoch [80/200], Train loss: 0.2128, Validation loss 0.1777\n",
      "Epoch [81/200], Train loss: 0.1442, Validation loss 0.1669\n",
      "Epoch [82/200], Train loss: 0.1384, Validation loss 0.1612\n",
      "Epoch [83/200], Train loss: 0.0759, Validation loss 0.1589\n",
      "Epoch [84/200], Train loss: 0.1517, Validation loss 0.1818\n",
      "Epoch [85/200], Train loss: 0.2367, Validation loss 0.1778\n",
      "Epoch [86/200], Train loss: 0.1548, Validation loss 0.1654\n",
      "Epoch [87/200], Train loss: 0.1567, Validation loss 0.1752\n",
      "Epoch [88/200], Train loss: 0.1159, Validation loss 0.1569\n",
      "Epoch [89/200], Train loss: 0.1471, Validation loss 0.1627\n",
      "Epoch [90/200], Train loss: 0.1964, Validation loss 0.1788\n",
      "Epoch [91/200], Train loss: 0.0906, Validation loss 0.1735\n",
      "Epoch [92/200], Train loss: 0.3031, Validation loss 0.1633\n",
      "Epoch [93/200], Train loss: 0.1469, Validation loss 0.1743\n",
      "Epoch [94/200], Train loss: 0.1390, Validation loss 0.1691\n",
      "Epoch [95/200], Train loss: 0.1296, Validation loss 0.1677\n",
      "Epoch [96/200], Train loss: 0.1990, Validation loss 0.1571\n",
      "Epoch [97/200], Train loss: 0.1485, Validation loss 0.1522\n",
      "Epoch [98/200], Train loss: 0.1866, Validation loss 0.1729\n",
      "Epoch [99/200], Train loss: 0.1670, Validation loss 0.1637\n",
      "Epoch [100/200], Train loss: 0.1645, Validation loss 0.1597\n",
      "Epoch [101/200], Train loss: 0.2051, Validation loss 0.1588\n",
      "Epoch [102/200], Train loss: 0.1467, Validation loss 0.1476\n",
      "Epoch [103/200], Train loss: 0.1590, Validation loss 0.1608\n",
      "Epoch [104/200], Train loss: 0.1289, Validation loss 0.1713\n",
      "Epoch [105/200], Train loss: 0.1750, Validation loss 0.1594\n",
      "Epoch [106/200], Train loss: 0.2165, Validation loss 0.1775\n",
      "Epoch [107/200], Train loss: 0.1598, Validation loss 0.2089\n",
      "Epoch [108/200], Train loss: 0.1452, Validation loss 0.1595\n",
      "Epoch [109/200], Train loss: 0.1380, Validation loss 0.1535\n",
      "Epoch [110/200], Train loss: 0.1445, Validation loss 0.1809\n",
      "Epoch [111/200], Train loss: 0.1499, Validation loss 0.1475\n",
      "Epoch [112/200], Train loss: 0.1248, Validation loss 0.1446\n",
      "Epoch [113/200], Train loss: 0.1957, Validation loss 0.1606\n",
      "Epoch [114/200], Train loss: 0.1714, Validation loss 0.1526\n",
      "Epoch [115/200], Train loss: 0.1422, Validation loss 0.1557\n",
      "Epoch [116/200], Train loss: 0.1119, Validation loss 0.1541\n",
      "Epoch [117/200], Train loss: 0.2707, Validation loss 0.1924\n",
      "Epoch [118/200], Train loss: 0.2816, Validation loss 0.1620\n",
      "Epoch [119/200], Train loss: 0.1292, Validation loss 0.1474\n",
      "Epoch [120/200], Train loss: 0.1346, Validation loss 0.1514\n",
      "Epoch [121/200], Train loss: 0.1105, Validation loss 0.1498\n",
      "Epoch [122/200], Train loss: 0.2338, Validation loss 0.1635\n",
      "Epoch [123/200], Train loss: 0.0561, Validation loss 0.1419\n",
      "Epoch [124/200], Train loss: 0.1860, Validation loss 0.1733\n",
      "Epoch [125/200], Train loss: 0.1394, Validation loss 0.1769\n",
      "Epoch [126/200], Train loss: 0.1047, Validation loss 0.1541\n",
      "Epoch [127/200], Train loss: 0.2159, Validation loss 0.1763\n",
      "Epoch [128/200], Train loss: 0.1353, Validation loss 0.1640\n",
      "Epoch [129/200], Train loss: 0.1535, Validation loss 0.1420\n",
      "Epoch [130/200], Train loss: 0.1721, Validation loss 0.1541\n",
      "Epoch [131/200], Train loss: 0.1678, Validation loss 0.1577\n",
      "Epoch [132/200], Train loss: 0.1617, Validation loss 0.1657\n",
      "Epoch [133/200], Train loss: 0.1187, Validation loss 0.1632\n",
      "Epoch [134/200], Train loss: 0.1284, Validation loss 0.1593\n",
      "Epoch [135/200], Train loss: 0.1312, Validation loss 0.1392\n",
      "Epoch [136/200], Train loss: 0.2315, Validation loss 0.1521\n",
      "Epoch [137/200], Train loss: 0.1886, Validation loss 0.1498\n",
      "Epoch [138/200], Train loss: 0.0981, Validation loss 0.1588\n",
      "Epoch [139/200], Train loss: 0.1599, Validation loss 0.1829\n",
      "Epoch [140/200], Train loss: 0.2916, Validation loss 0.1562\n",
      "Epoch [141/200], Train loss: 0.0852, Validation loss 0.1462\n",
      "Epoch [142/200], Train loss: 0.2087, Validation loss 0.1438\n",
      "Epoch [143/200], Train loss: 0.4441, Validation loss 0.2611\n",
      "Epoch [144/200], Train loss: 0.1732, Validation loss 0.1424\n",
      "Epoch [145/200], Train loss: 0.1243, Validation loss 0.1504\n",
      "Epoch [146/200], Train loss: 0.1946, Validation loss 0.1725\n",
      "Epoch [147/200], Train loss: 0.0787, Validation loss 0.1719\n",
      "Epoch [148/200], Train loss: 0.1027, Validation loss 0.1585\n",
      "Epoch [149/200], Train loss: 0.1558, Validation loss 0.1432\n",
      "Epoch [150/200], Train loss: 0.1512, Validation loss 0.1387\n",
      "Epoch [151/200], Train loss: 0.1607, Validation loss 0.1618\n",
      "Epoch [152/200], Train loss: 0.1324, Validation loss 0.1452\n",
      "Epoch [153/200], Train loss: 0.1484, Validation loss 0.1494\n",
      "Epoch [154/200], Train loss: 0.1957, Validation loss 0.1543\n",
      "Epoch [155/200], Train loss: 0.1525, Validation loss 0.1754\n",
      "Epoch [156/200], Train loss: 0.1136, Validation loss 0.1593\n",
      "Epoch [157/200], Train loss: 0.2031, Validation loss 0.1502\n",
      "Epoch [158/200], Train loss: 0.1656, Validation loss 0.1683\n",
      "Epoch [159/200], Train loss: 0.1194, Validation loss 0.1595\n",
      "Epoch [160/200], Train loss: 0.1161, Validation loss 0.1399\n",
      "Epoch [161/200], Train loss: 0.2292, Validation loss 0.1490\n",
      "Epoch [162/200], Train loss: 0.1521, Validation loss 0.1681\n",
      "Epoch [163/200], Train loss: 0.2211, Validation loss 0.1602\n",
      "Epoch [164/200], Train loss: 0.1921, Validation loss 0.1455\n",
      "Epoch [165/200], Train loss: 0.1650, Validation loss 0.1529\n",
      "Epoch [166/200], Train loss: 0.0949, Validation loss 0.1517\n",
      "Epoch [167/200], Train loss: 0.1546, Validation loss 0.1392\n",
      "Epoch [168/200], Train loss: 0.0743, Validation loss 0.1519\n",
      "Epoch [169/200], Train loss: 0.1892, Validation loss 0.1373\n",
      "Epoch [170/200], Train loss: 0.1581, Validation loss 0.1576\n",
      "Epoch [171/200], Train loss: 0.1807, Validation loss 0.1451\n",
      "Epoch [172/200], Train loss: 0.1165, Validation loss 0.1427\n",
      "Epoch [173/200], Train loss: 0.1264, Validation loss 0.1593\n",
      "Epoch [174/200], Train loss: 0.1548, Validation loss 0.1455\n",
      "Epoch [175/200], Train loss: 0.1743, Validation loss 0.1665\n",
      "Epoch [176/200], Train loss: 0.1988, Validation loss 0.1428\n",
      "Epoch [177/200], Train loss: 0.0856, Validation loss 0.1674\n",
      "Epoch [178/200], Train loss: 0.1557, Validation loss 0.1586\n",
      "Epoch [179/200], Train loss: 0.1580, Validation loss 0.1752\n",
      "Epoch [180/200], Train loss: 0.2448, Validation loss 0.1712\n",
      "Epoch [181/200], Train loss: 0.0832, Validation loss 0.1476\n",
      "Epoch [182/200], Train loss: 0.1853, Validation loss 0.1573\n",
      "Epoch [183/200], Train loss: 0.0918, Validation loss 0.1473\n",
      "Epoch [184/200], Train loss: 0.1244, Validation loss 0.1374\n",
      "Epoch [185/200], Train loss: 0.1482, Validation loss 0.1492\n",
      "Epoch [186/200], Train loss: 0.0276, Validation loss 0.1411\n",
      "Epoch [187/200], Train loss: 0.0923, Validation loss 0.1358\n",
      "Epoch [188/200], Train loss: 0.1595, Validation loss 0.1547\n",
      "Epoch [189/200], Train loss: 0.1575, Validation loss 0.1681\n",
      "Epoch [190/200], Train loss: 0.0955, Validation loss 0.1678\n",
      "Epoch [191/200], Train loss: 0.0490, Validation loss 0.1497\n",
      "Epoch [192/200], Train loss: 0.0979, Validation loss 0.1580\n",
      "Epoch [193/200], Train loss: 0.2159, Validation loss 0.1443\n",
      "Epoch [194/200], Train loss: 0.0899, Validation loss 0.1908\n",
      "Epoch [195/200], Train loss: 0.1317, Validation loss 0.1386\n",
      "Epoch [196/200], Train loss: 0.1523, Validation loss 0.1350\n",
      "Epoch [197/200], Train loss: 0.1741, Validation loss 0.1455\n",
      "Epoch [198/200], Train loss: 0.1628, Validation loss 0.1310\n",
      "Epoch [199/200], Train loss: 0.1304, Validation loss 0.1542\n",
      "Epoch [200/200], Train loss: 0.1145, Validation loss 0.1445\n",
      "Training finished! after 199 epochs, Model saved to ./model-fold-2.pth\n",
      "Epoch [1/200], Train loss: 1.3120, Validation loss 1.3212\n",
      "Epoch [2/200], Train loss: 1.2839, Validation loss 1.2651\n",
      "Epoch [3/200], Train loss: 1.1662, Validation loss 1.1936\n",
      "Epoch [4/200], Train loss: 1.1336, Validation loss 1.1276\n",
      "Epoch [5/200], Train loss: 1.0367, Validation loss 1.0737\n",
      "Epoch [6/200], Train loss: 1.0819, Validation loss 1.0543\n",
      "Epoch [7/200], Train loss: 0.9607, Validation loss 1.0133\n",
      "Epoch [8/200], Train loss: 0.9223, Validation loss 0.9762\n",
      "Epoch [9/200], Train loss: 0.9538, Validation loss 0.9605\n",
      "Epoch [10/200], Train loss: 0.9594, Validation loss 0.9216\n",
      "Epoch [11/200], Train loss: 0.9701, Validation loss 0.8432\n",
      "Epoch [12/200], Train loss: 0.8177, Validation loss 0.8129\n",
      "Epoch [13/200], Train loss: 0.7891, Validation loss 0.7755\n",
      "Epoch [14/200], Train loss: 0.8524, Validation loss 0.7538\n",
      "Epoch [15/200], Train loss: 0.6704, Validation loss 0.7165\n",
      "Epoch [16/200], Train loss: 0.6478, Validation loss 0.7021\n",
      "Epoch [17/200], Train loss: 0.7094, Validation loss 0.6891\n",
      "Epoch [18/200], Train loss: 0.5643, Validation loss 0.6161\n",
      "Epoch [19/200], Train loss: 0.6022, Validation loss 0.6404\n",
      "Epoch [20/200], Train loss: 0.6237, Validation loss 0.6143\n",
      "Epoch [21/200], Train loss: 0.4326, Validation loss 0.5059\n",
      "Epoch [22/200], Train loss: 0.5325, Validation loss 0.5524\n",
      "Epoch [23/200], Train loss: 0.4213, Validation loss 0.4947\n",
      "Epoch [24/200], Train loss: 0.5103, Validation loss 0.5317\n",
      "Epoch [25/200], Train loss: 0.4980, Validation loss 0.4928\n",
      "Epoch [26/200], Train loss: 0.5204, Validation loss 0.4873\n",
      "Epoch [27/200], Train loss: 0.4098, Validation loss 0.4443\n",
      "Epoch [28/200], Train loss: 0.3490, Validation loss 0.4489\n",
      "Epoch [29/200], Train loss: 0.4424, Validation loss 0.4490\n",
      "Epoch [30/200], Train loss: 0.4561, Validation loss 0.4587\n",
      "Epoch [31/200], Train loss: 0.4837, Validation loss 0.4260\n",
      "Epoch [32/200], Train loss: 0.3693, Validation loss 0.3663\n",
      "Epoch [33/200], Train loss: 0.3953, Validation loss 0.3892\n",
      "Epoch [34/200], Train loss: 0.3739, Validation loss 0.3994\n",
      "Epoch [35/200], Train loss: 0.3693, Validation loss 0.4303\n",
      "Epoch [36/200], Train loss: 0.3601, Validation loss 0.3684\n",
      "Epoch [37/200], Train loss: 0.3354, Validation loss 0.4529\n",
      "Epoch [38/200], Train loss: 0.3330, Validation loss 0.3269\n",
      "Epoch [39/200], Train loss: 0.2548, Validation loss 0.3090\n",
      "Epoch [40/200], Train loss: 0.2851, Validation loss 0.3042\n",
      "Epoch [41/200], Train loss: 0.2776, Validation loss 0.3135\n",
      "Epoch [42/200], Train loss: 0.2517, Validation loss 0.2750\n",
      "Epoch [43/200], Train loss: 0.2931, Validation loss 0.3899\n",
      "Epoch [44/200], Train loss: 0.2903, Validation loss 0.2707\n",
      "Epoch [45/200], Train loss: 0.2551, Validation loss 0.3026\n",
      "Epoch [46/200], Train loss: 0.1983, Validation loss 0.2927\n",
      "Epoch [47/200], Train loss: 0.2225, Validation loss 0.2931\n",
      "Epoch [48/200], Train loss: 0.3671, Validation loss 0.2666\n",
      "Epoch [49/200], Train loss: 0.3074, Validation loss 0.3009\n",
      "Epoch [50/200], Train loss: 0.3459, Validation loss 0.4010\n",
      "Epoch [51/200], Train loss: 0.2725, Validation loss 0.2414\n",
      "Epoch [52/200], Train loss: 0.2589, Validation loss 0.2838\n",
      "Epoch [53/200], Train loss: 0.2360, Validation loss 0.2716\n",
      "Epoch [54/200], Train loss: 0.2446, Validation loss 0.2335\n",
      "Epoch [55/200], Train loss: 0.2318, Validation loss 0.2264\n",
      "Epoch [56/200], Train loss: 0.2177, Validation loss 0.2888\n",
      "Epoch [57/200], Train loss: 0.1889, Validation loss 0.3091\n",
      "Epoch [58/200], Train loss: 0.2208, Validation loss 0.3516\n",
      "Epoch [59/200], Train loss: 0.3127, Validation loss 0.2377\n",
      "Epoch [60/200], Train loss: 0.1783, Validation loss 0.2304\n",
      "Epoch [61/200], Train loss: 0.1263, Validation loss 0.3220\n",
      "Epoch [62/200], Train loss: 0.2427, Validation loss 0.2750\n",
      "Epoch [63/200], Train loss: 0.1477, Validation loss 0.2799\n",
      "Epoch [64/200], Train loss: 0.3207, Validation loss 0.2827\n",
      "Epoch [65/200], Train loss: 0.2672, Validation loss 0.2279\n",
      "Epoch [66/200], Train loss: 0.4084, Validation loss 0.3794\n",
      "Epoch [67/200], Train loss: 0.4123, Validation loss 0.3140\n",
      "Epoch [68/200], Train loss: 0.1313, Validation loss 0.2816\n",
      "Epoch [69/200], Train loss: 0.2564, Validation loss 0.2406\n",
      "Epoch [70/200], Train loss: 0.2075, Validation loss 0.3607\n",
      "Epoch [71/200], Train loss: 0.3515, Validation loss 0.2752\n",
      "Epoch [72/200], Train loss: 0.4860, Validation loss 0.3000\n",
      "Epoch [73/200], Train loss: 0.1232, Validation loss 0.1531\n",
      "Epoch [74/200], Train loss: 0.2323, Validation loss 0.2878\n",
      "Epoch [75/200], Train loss: 0.2789, Validation loss 0.2753\n",
      "Epoch [76/200], Train loss: 0.2493, Validation loss 0.3155\n",
      "Epoch [77/200], Train loss: 0.3752, Validation loss 0.3042\n",
      "Epoch [78/200], Train loss: 0.1598, Validation loss 0.1641\n",
      "Epoch [79/200], Train loss: 0.2012, Validation loss 0.2361\n",
      "Epoch [80/200], Train loss: 0.3196, Validation loss 0.2308\n",
      "Epoch [81/200], Train loss: 0.5427, Validation loss 0.3798\n",
      "Epoch [82/200], Train loss: 0.1229, Validation loss 0.1976\n",
      "Epoch [83/200], Train loss: 0.2486, Validation loss 0.2263\n",
      "Epoch [84/200], Train loss: 0.2702, Validation loss 0.2428\n",
      "Epoch [85/200], Train loss: 0.3225, Validation loss 0.5257\n",
      "Epoch [86/200], Train loss: 0.2260, Validation loss 0.1866\n",
      "Epoch [87/200], Train loss: 0.1299, Validation loss 0.1605\n",
      "Epoch [88/200], Train loss: 0.1762, Validation loss 0.1474\n",
      "Epoch [89/200], Train loss: 0.2024, Validation loss 0.1859\n",
      "Epoch [90/200], Train loss: 0.2925, Validation loss 0.2190\n",
      "Epoch [91/200], Train loss: 0.2136, Validation loss 0.1458\n",
      "Epoch [92/200], Train loss: 0.1618, Validation loss 0.2740\n",
      "Epoch [93/200], Train loss: 0.4543, Validation loss 0.2190\n",
      "Epoch [94/200], Train loss: 0.2240, Validation loss 0.1678\n",
      "Epoch [95/200], Train loss: 0.0862, Validation loss 0.2415\n",
      "Epoch [96/200], Train loss: 0.0557, Validation loss 0.3520\n",
      "Epoch [97/200], Train loss: 0.1959, Validation loss 0.1439\n",
      "Epoch [98/200], Train loss: 0.3546, Validation loss 0.2254\n",
      "Epoch [99/200], Train loss: 0.1875, Validation loss 0.1364\n",
      "Epoch [100/200], Train loss: 0.0683, Validation loss 0.1629\n",
      "Epoch [101/200], Train loss: 0.1773, Validation loss 0.1467\n",
      "Epoch [102/200], Train loss: 0.1348, Validation loss 0.1530\n",
      "Epoch [103/200], Train loss: 0.1794, Validation loss 0.1703\n",
      "Epoch [104/200], Train loss: 0.1523, Validation loss 0.2144\n",
      "Epoch [105/200], Train loss: 0.1143, Validation loss 0.1743\n",
      "Epoch [106/200], Train loss: 0.1514, Validation loss 0.1549\n",
      "Epoch [107/200], Train loss: 0.3360, Validation loss 0.1904\n",
      "Epoch [108/200], Train loss: 0.2518, Validation loss 0.2165\n",
      "Epoch [109/200], Train loss: 0.1628, Validation loss 0.1604\n",
      "Epoch [110/200], Train loss: 0.1382, Validation loss 0.1825\n",
      "Epoch [111/200], Train loss: 0.1209, Validation loss 0.1427\n",
      "Epoch [112/200], Train loss: 0.5403, Validation loss 0.2013\n",
      "Epoch [113/200], Train loss: 0.1435, Validation loss 0.1676\n",
      "Epoch [114/200], Train loss: 0.0875, Validation loss 0.2037\n",
      "Epoch [115/200], Train loss: 0.2996, Validation loss 0.1680\n",
      "Epoch [116/200], Train loss: 0.1006, Validation loss 0.1513\n",
      "Epoch [117/200], Train loss: 0.1241, Validation loss 0.2298\n",
      "Epoch [118/200], Train loss: 0.1380, Validation loss 0.1862\n",
      "Training finished! after 118 epochs, Model saved to ./model-fold-3.pth\n",
      "Epoch [1/200], Train loss: 0.8703, Validation loss 1.0143\n",
      "Epoch [2/200], Train loss: 0.9102, Validation loss 0.8762\n",
      "Epoch [3/200], Train loss: 0.8975, Validation loss 0.7648\n",
      "Epoch [4/200], Train loss: 0.7200, Validation loss 0.7331\n",
      "Epoch [5/200], Train loss: 0.5913, Validation loss 0.6713\n",
      "Epoch [6/200], Train loss: 0.5475, Validation loss 0.6404\n",
      "Epoch [7/200], Train loss: 0.7106, Validation loss 0.5799\n",
      "Epoch [8/200], Train loss: 0.4123, Validation loss 0.5328\n",
      "Epoch [9/200], Train loss: 0.5129, Validation loss 0.5120\n",
      "Epoch [10/200], Train loss: 0.4854, Validation loss 0.4953\n",
      "Epoch [11/200], Train loss: 0.3982, Validation loss 0.4692\n",
      "Epoch [12/200], Train loss: 0.5489, Validation loss 0.4994\n",
      "Epoch [13/200], Train loss: 0.4368, Validation loss 0.4438\n",
      "Epoch [14/200], Train loss: 0.5263, Validation loss 0.4369\n",
      "Epoch [15/200], Train loss: 0.3121, Validation loss 0.3778\n",
      "Epoch [16/200], Train loss: 0.3318, Validation loss 0.3656\n",
      "Epoch [17/200], Train loss: 0.3530, Validation loss 0.3535\n",
      "Epoch [18/200], Train loss: 0.2936, Validation loss 0.3545\n",
      "Epoch [19/200], Train loss: 0.2318, Validation loss 0.3505\n",
      "Epoch [20/200], Train loss: 0.4212, Validation loss 0.3720\n",
      "Epoch [21/200], Train loss: 0.1992, Validation loss 0.3156\n",
      "Epoch [22/200], Train loss: 0.2576, Validation loss 0.2975\n",
      "Epoch [23/200], Train loss: 0.2579, Validation loss 0.3157\n",
      "Epoch [24/200], Train loss: 0.3366, Validation loss 0.7269\n",
      "Epoch [25/200], Train loss: 0.2004, Validation loss 0.3515\n",
      "Epoch [26/200], Train loss: 0.1468, Validation loss 0.2858\n",
      "Epoch [27/200], Train loss: 0.5144, Validation loss 0.3311\n",
      "Epoch [28/200], Train loss: 0.5308, Validation loss 0.3261\n",
      "Epoch [29/200], Train loss: 0.2822, Validation loss 0.2781\n",
      "Epoch [30/200], Train loss: 0.4004, Validation loss 0.3056\n",
      "Epoch [31/200], Train loss: 0.2820, Validation loss 0.3340\n",
      "Epoch [32/200], Train loss: 0.2061, Validation loss 0.3287\n",
      "Epoch [33/200], Train loss: 0.4885, Validation loss 0.3148\n",
      "Epoch [34/200], Train loss: 0.6614, Validation loss 0.3762\n",
      "Epoch [35/200], Train loss: 0.0970, Validation loss 0.2598\n",
      "Epoch [36/200], Train loss: 0.3267, Validation loss 0.2803\n",
      "Epoch [37/200], Train loss: 0.1405, Validation loss 0.2445\n",
      "Epoch [38/200], Train loss: 0.2621, Validation loss 0.2257\n",
      "Epoch [39/200], Train loss: 0.2754, Validation loss 0.2916\n",
      "Epoch [40/200], Train loss: 0.1409, Validation loss 0.2389\n",
      "Epoch [41/200], Train loss: 0.3080, Validation loss 0.3058\n",
      "Epoch [42/200], Train loss: 0.3861, Validation loss 0.2762\n",
      "Epoch [43/200], Train loss: 0.1210, Validation loss 0.2151\n",
      "Epoch [44/200], Train loss: 0.6691, Validation loss 0.3669\n",
      "Epoch [45/200], Train loss: 0.3501, Validation loss 0.2498\n",
      "Epoch [46/200], Train loss: 0.2761, Validation loss 0.2294\n",
      "Epoch [47/200], Train loss: 0.2614, Validation loss 0.2845\n",
      "Epoch [48/200], Train loss: 0.2223, Validation loss 0.2641\n",
      "Epoch [49/200], Train loss: 0.8657, Validation loss 1.1393\n",
      "Epoch [50/200], Train loss: 0.4469, Validation loss 0.2187\n",
      "Epoch [51/200], Train loss: 0.0413, Validation loss 0.2002\n",
      "Epoch [52/200], Train loss: 0.0872, Validation loss 0.2239\n",
      "Epoch [53/200], Train loss: 0.5417, Validation loss 0.2155\n",
      "Epoch [54/200], Train loss: 0.2265, Validation loss 0.2073\n",
      "Epoch [55/200], Train loss: 0.1553, Validation loss 0.1766\n",
      "Epoch [56/200], Train loss: 0.4478, Validation loss 0.2226\n",
      "Epoch [57/200], Train loss: 0.4813, Validation loss 0.2585\n",
      "Epoch [58/200], Train loss: 0.3625, Validation loss 0.2034\n",
      "Epoch [59/200], Train loss: 0.4234, Validation loss 0.2041\n",
      "Epoch [60/200], Train loss: 0.2031, Validation loss 0.2473\n",
      "Epoch [61/200], Train loss: 0.1279, Validation loss 0.2064\n",
      "Epoch [62/200], Train loss: 0.2120, Validation loss 0.2826\n",
      "Epoch [63/200], Train loss: 0.1151, Validation loss 0.2064\n",
      "Epoch [64/200], Train loss: 0.1131, Validation loss 0.2339\n",
      "Epoch [65/200], Train loss: 0.1432, Validation loss 0.1904\n",
      "Epoch [66/200], Train loss: 0.0657, Validation loss 0.1978\n",
      "Epoch [67/200], Train loss: 0.2951, Validation loss 0.1898\n",
      "Epoch [68/200], Train loss: 0.3697, Validation loss 0.3095\n",
      "Epoch [69/200], Train loss: 0.2696, Validation loss 0.3789\n",
      "Epoch [70/200], Train loss: 0.5392, Validation loss 0.2339\n",
      "Epoch [71/200], Train loss: 0.1411, Validation loss 0.2564\n",
      "Epoch [72/200], Train loss: 0.0888, Validation loss 0.2253\n",
      "Epoch [73/200], Train loss: 0.5683, Validation loss 0.1889\n",
      "Epoch [74/200], Train loss: 0.1526, Validation loss 0.2601\n",
      "Training finished! after 74 epochs, Model saved to ./model-fold-4.pth\n",
      "Epoch [1/200], Train loss: 1.2696, Validation loss 1.2039\n",
      "Epoch [2/200], Train loss: 1.0397, Validation loss 1.0874\n",
      "Epoch [3/200], Train loss: 1.1115, Validation loss 1.0196\n",
      "Epoch [4/200], Train loss: 0.8927, Validation loss 0.9528\n",
      "Epoch [5/200], Train loss: 1.0380, Validation loss 0.9513\n",
      "Epoch [6/200], Train loss: 0.8058, Validation loss 0.8503\n",
      "Epoch [7/200], Train loss: 0.8653, Validation loss 0.8066\n",
      "Epoch [8/200], Train loss: 1.0618, Validation loss 0.7955\n",
      "Epoch [9/200], Train loss: 0.6022, Validation loss 0.6533\n",
      "Epoch [10/200], Train loss: 0.5660, Validation loss 0.6786\n",
      "Epoch [11/200], Train loss: 0.5980, Validation loss 0.5979\n",
      "Epoch [12/200], Train loss: 0.5924, Validation loss 0.5718\n",
      "Epoch [13/200], Train loss: 0.5540, Validation loss 0.5935\n",
      "Epoch [14/200], Train loss: 0.5799, Validation loss 0.5212\n",
      "Epoch [15/200], Train loss: 0.4713, Validation loss 0.4831\n",
      "Epoch [16/200], Train loss: 0.5028, Validation loss 0.4842\n",
      "Epoch [17/200], Train loss: 0.5442, Validation loss 0.4671\n",
      "Epoch [18/200], Train loss: 0.5366, Validation loss 0.4911\n",
      "Epoch [19/200], Train loss: 0.4700, Validation loss 0.4487\n",
      "Epoch [20/200], Train loss: 0.4648, Validation loss 0.4024\n",
      "Epoch [21/200], Train loss: 0.5459, Validation loss 0.4087\n",
      "Epoch [22/200], Train loss: 0.3539, Validation loss 0.3859\n",
      "Epoch [23/200], Train loss: 0.4220, Validation loss 0.4194\n",
      "Epoch [24/200], Train loss: 0.2776, Validation loss 0.4098\n",
      "Epoch [25/200], Train loss: 0.5157, Validation loss 0.3630\n",
      "Epoch [26/200], Train loss: 0.3191, Validation loss 0.3485\n",
      "Epoch [27/200], Train loss: 0.2710, Validation loss 0.3614\n",
      "Epoch [28/200], Train loss: 0.3328, Validation loss 0.3197\n",
      "Epoch [29/200], Train loss: 0.2961, Validation loss 0.3254\n",
      "Epoch [30/200], Train loss: 0.3346, Validation loss 0.3070\n",
      "Epoch [31/200], Train loss: 0.2310, Validation loss 0.2969\n",
      "Epoch [32/200], Train loss: 0.1777, Validation loss 0.3154\n",
      "Epoch [33/200], Train loss: 0.3562, Validation loss 0.3027\n",
      "Epoch [34/200], Train loss: 0.2151, Validation loss 0.2875\n",
      "Epoch [35/200], Train loss: 0.2887, Validation loss 0.2830\n",
      "Epoch [36/200], Train loss: 0.2183, Validation loss 0.2871\n",
      "Epoch [37/200], Train loss: 0.2976, Validation loss 0.2757\n",
      "Epoch [38/200], Train loss: 0.1837, Validation loss 0.2750\n",
      "Epoch [39/200], Train loss: 0.1798, Validation loss 0.2555\n",
      "Epoch [40/200], Train loss: 0.2495, Validation loss 0.2910\n",
      "Epoch [41/200], Train loss: 0.2308, Validation loss 0.2656\n",
      "Epoch [42/200], Train loss: 0.2326, Validation loss 0.2631\n",
      "Epoch [43/200], Train loss: 0.1954, Validation loss 0.2701\n",
      "Epoch [44/200], Train loss: 0.3110, Validation loss 0.2573\n",
      "Epoch [45/200], Train loss: 0.2795, Validation loss 0.2438\n",
      "Epoch [46/200], Train loss: 0.1970, Validation loss 0.2454\n",
      "Epoch [47/200], Train loss: 0.1565, Validation loss 0.2297\n",
      "Epoch [48/200], Train loss: 0.2320, Validation loss 0.2310\n",
      "Epoch [49/200], Train loss: 0.0988, Validation loss 0.2356\n",
      "Epoch [50/200], Train loss: 0.2077, Validation loss 0.2116\n",
      "Epoch [51/200], Train loss: 0.1684, Validation loss 0.2282\n",
      "Epoch [52/200], Train loss: 0.1730, Validation loss 0.2232\n",
      "Epoch [53/200], Train loss: 0.5379, Validation loss 0.2408\n",
      "Epoch [54/200], Train loss: 0.2708, Validation loss 0.2084\n",
      "Epoch [55/200], Train loss: 0.2083, Validation loss 0.2288\n",
      "Epoch [56/200], Train loss: 0.2420, Validation loss 0.2155\n",
      "Epoch [57/200], Train loss: 0.1956, Validation loss 0.2198\n",
      "Epoch [58/200], Train loss: 0.2896, Validation loss 0.2197\n",
      "Epoch [59/200], Train loss: 0.2002, Validation loss 0.2157\n",
      "Epoch [60/200], Train loss: 0.3160, Validation loss 0.2230\n",
      "Epoch [61/200], Train loss: 0.3567, Validation loss 0.2198\n",
      "Epoch [62/200], Train loss: 0.2431, Validation loss 0.2338\n",
      "Epoch [63/200], Train loss: 0.6325, Validation loss 0.2344\n",
      "Epoch [64/200], Train loss: 0.2115, Validation loss 0.2009\n",
      "Epoch [65/200], Train loss: 0.1712, Validation loss 0.2047\n",
      "Epoch [66/200], Train loss: 0.1073, Validation loss 0.2039\n",
      "Epoch [67/200], Train loss: 0.2382, Validation loss 0.2358\n",
      "Epoch [68/200], Train loss: 0.1905, Validation loss 0.1964\n",
      "Epoch [69/200], Train loss: 0.2696, Validation loss 0.2198\n",
      "Epoch [70/200], Train loss: 0.2230, Validation loss 0.2200\n",
      "Epoch [71/200], Train loss: 0.2592, Validation loss 0.2238\n",
      "Epoch [72/200], Train loss: 0.1798, Validation loss 0.2115\n",
      "Epoch [73/200], Train loss: 0.3382, Validation loss 0.2128\n",
      "Epoch [74/200], Train loss: 0.4270, Validation loss 0.2107\n",
      "Epoch [75/200], Train loss: 0.1942, Validation loss 0.2263\n",
      "Epoch [76/200], Train loss: 0.1205, Validation loss 0.1941\n",
      "Epoch [77/200], Train loss: 0.2081, Validation loss 0.1881\n",
      "Epoch [78/200], Train loss: 0.2251, Validation loss 0.1892\n",
      "Epoch [79/200], Train loss: 0.1026, Validation loss 0.1786\n",
      "Epoch [80/200], Train loss: 0.3010, Validation loss 0.1940\n",
      "Epoch [81/200], Train loss: 0.2413, Validation loss 0.1807\n",
      "Epoch [82/200], Train loss: 0.1649, Validation loss 0.1902\n",
      "Epoch [83/200], Train loss: 0.2208, Validation loss 0.2037\n",
      "Epoch [84/200], Train loss: 0.1779, Validation loss 0.1811\n",
      "Epoch [85/200], Train loss: 0.3172, Validation loss 0.2080\n",
      "Epoch [86/200], Train loss: 0.1002, Validation loss 0.2447\n",
      "Epoch [87/200], Train loss: 0.2397, Validation loss 0.2116\n",
      "Epoch [88/200], Train loss: 0.1836, Validation loss 0.1867\n",
      "Epoch [89/200], Train loss: 0.1601, Validation loss 0.2276\n",
      "Epoch [90/200], Train loss: 0.2574, Validation loss 0.2001\n",
      "Epoch [91/200], Train loss: 0.1512, Validation loss 0.1805\n",
      "Epoch [92/200], Train loss: 0.1058, Validation loss 0.2223\n",
      "Epoch [93/200], Train loss: 0.1912, Validation loss 0.1904\n",
      "Epoch [94/200], Train loss: 0.1182, Validation loss 0.1802\n",
      "Epoch [95/200], Train loss: 0.0497, Validation loss 0.1613\n",
      "Epoch [96/200], Train loss: 0.0417, Validation loss 0.1632\n",
      "Epoch [97/200], Train loss: 0.1033, Validation loss 0.1987\n",
      "Epoch [98/200], Train loss: 0.2136, Validation loss 0.2022\n",
      "Epoch [99/200], Train loss: 0.2084, Validation loss 0.1927\n",
      "Epoch [100/200], Train loss: 0.2303, Validation loss 0.2383\n",
      "Epoch [101/200], Train loss: 0.3304, Validation loss 0.1771\n",
      "Epoch [102/200], Train loss: 0.2445, Validation loss 0.1665\n",
      "Epoch [103/200], Train loss: 0.0794, Validation loss 0.1745\n",
      "Epoch [104/200], Train loss: 0.2292, Validation loss 0.1992\n",
      "Epoch [105/200], Train loss: 0.3127, Validation loss 0.2018\n",
      "Epoch [106/200], Train loss: 0.1701, Validation loss 0.1905\n",
      "Epoch [107/200], Train loss: 0.2516, Validation loss 0.1885\n",
      "Epoch [108/200], Train loss: 0.1531, Validation loss 0.1690\n",
      "Epoch [109/200], Train loss: 0.1153, Validation loss 0.1600\n",
      "Epoch [110/200], Train loss: 0.1283, Validation loss 0.1784\n",
      "Epoch [111/200], Train loss: 0.1754, Validation loss 0.1660\n",
      "Epoch [112/200], Train loss: 0.1481, Validation loss 0.2408\n",
      "Epoch [113/200], Train loss: 0.2063, Validation loss 0.1910\n",
      "Epoch [114/200], Train loss: 0.2032, Validation loss 0.2255\n",
      "Epoch [115/200], Train loss: 0.1584, Validation loss 0.1685\n",
      "Epoch [116/200], Train loss: 0.1941, Validation loss 0.2251\n",
      "Epoch [117/200], Train loss: 0.1532, Validation loss 0.1823\n",
      "Epoch [118/200], Train loss: 0.1535, Validation loss 0.1754\n",
      "Epoch [119/200], Train loss: 0.1488, Validation loss 0.1915\n",
      "Epoch [120/200], Train loss: 0.2119, Validation loss 0.1960\n",
      "Epoch [121/200], Train loss: 0.1180, Validation loss 0.1680\n",
      "Epoch [122/200], Train loss: 0.1359, Validation loss 0.2444\n",
      "Epoch [123/200], Train loss: 0.7146, Validation loss 0.2479\n",
      "Epoch [124/200], Train loss: 0.1643, Validation loss 0.1829\n",
      "Epoch [125/200], Train loss: 0.0995, Validation loss 0.1703\n",
      "Epoch [126/200], Train loss: 0.1777, Validation loss 0.1706\n",
      "Epoch [127/200], Train loss: 0.2057, Validation loss 0.1636\n",
      "Epoch [128/200], Train loss: 0.2169, Validation loss 0.1867\n",
      "Training finished! after 128 epochs, Model saved to ./model-fold-5.pth\n",
      "Epoch [1/200], Train loss: 1.1463, Validation loss 1.1684\n",
      "Epoch [2/200], Train loss: 1.1412, Validation loss 1.0815\n",
      "Epoch [3/200], Train loss: 1.1314, Validation loss 1.0135\n",
      "Epoch [4/200], Train loss: 0.8926, Validation loss 0.9322\n",
      "Epoch [5/200], Train loss: 0.7645, Validation loss 0.8551\n",
      "Epoch [6/200], Train loss: 0.7630, Validation loss 0.8204\n",
      "Epoch [7/200], Train loss: 0.7657, Validation loss 0.7771\n",
      "Epoch [8/200], Train loss: 0.7397, Validation loss 0.7166\n",
      "Epoch [9/200], Train loss: 0.6359, Validation loss 0.7776\n",
      "Epoch [10/200], Train loss: 0.6072, Validation loss 0.6518\n",
      "Epoch [11/200], Train loss: 0.6424, Validation loss 0.6800\n",
      "Epoch [12/200], Train loss: 0.5735, Validation loss 0.6017\n",
      "Epoch [13/200], Train loss: 0.6757, Validation loss 0.6533\n",
      "Epoch [14/200], Train loss: 0.5618, Validation loss 0.5808\n",
      "Epoch [15/200], Train loss: 0.6026, Validation loss 0.5610\n",
      "Epoch [16/200], Train loss: 0.5728, Validation loss 0.6036\n",
      "Epoch [17/200], Train loss: 0.6132, Validation loss 0.5327\n",
      "Epoch [18/200], Train loss: 0.4475, Validation loss 0.4755\n",
      "Epoch [19/200], Train loss: 0.3927, Validation loss 0.5185\n",
      "Epoch [20/200], Train loss: 0.4993, Validation loss 0.4933\n",
      "Epoch [21/200], Train loss: 0.4997, Validation loss 0.4563\n",
      "Epoch [22/200], Train loss: 0.3147, Validation loss 0.4093\n",
      "Epoch [23/200], Train loss: 0.5998, Validation loss 0.5919\n",
      "Epoch [24/200], Train loss: 0.3717, Validation loss 0.4150\n",
      "Epoch [25/200], Train loss: 0.2888, Validation loss 0.3968\n",
      "Epoch [26/200], Train loss: 0.2360, Validation loss 0.3750\n",
      "Epoch [27/200], Train loss: 0.2222, Validation loss 0.3742\n",
      "Epoch [28/200], Train loss: 0.4040, Validation loss 0.3938\n",
      "Epoch [29/200], Train loss: 0.3043, Validation loss 0.3629\n",
      "Epoch [30/200], Train loss: 0.3036, Validation loss 0.4078\n",
      "Epoch [31/200], Train loss: 0.3007, Validation loss 0.3285\n",
      "Epoch [32/200], Train loss: 0.4344, Validation loss 0.3380\n",
      "Epoch [33/200], Train loss: 0.5718, Validation loss 0.3640\n",
      "Epoch [34/200], Train loss: 0.2836, Validation loss 0.3533\n",
      "Epoch [35/200], Train loss: 0.2851, Validation loss 0.3048\n",
      "Epoch [36/200], Train loss: 0.3513, Validation loss 0.3572\n",
      "Epoch [37/200], Train loss: 0.3032, Validation loss 0.3301\n",
      "Epoch [38/200], Train loss: 0.2881, Validation loss 0.3558\n",
      "Epoch [39/200], Train loss: 0.3111, Validation loss 0.3235\n",
      "Epoch [40/200], Train loss: 0.3410, Validation loss 0.2862\n",
      "Epoch [41/200], Train loss: 0.1778, Validation loss 0.2974\n",
      "Epoch [42/200], Train loss: 0.2475, Validation loss 0.2822\n",
      "Epoch [43/200], Train loss: 0.2066, Validation loss 0.3271\n",
      "Epoch [44/200], Train loss: 0.1982, Validation loss 0.3404\n",
      "Epoch [45/200], Train loss: 0.2177, Validation loss 0.2900\n",
      "Epoch [46/200], Train loss: 0.4146, Validation loss 0.2868\n",
      "Epoch [47/200], Train loss: 0.1512, Validation loss 0.3007\n",
      "Epoch [48/200], Train loss: 0.1789, Validation loss 0.2597\n",
      "Epoch [49/200], Train loss: 0.1526, Validation loss 0.2628\n",
      "Epoch [50/200], Train loss: 0.1957, Validation loss 0.2947\n",
      "Epoch [51/200], Train loss: 0.1922, Validation loss 0.3313\n",
      "Epoch [52/200], Train loss: 0.2373, Validation loss 0.2474\n",
      "Epoch [53/200], Train loss: 0.2193, Validation loss 0.2461\n",
      "Epoch [54/200], Train loss: 0.2911, Validation loss 0.2853\n",
      "Epoch [55/200], Train loss: 0.2482, Validation loss 0.2602\n",
      "Epoch [56/200], Train loss: 0.7024, Validation loss 0.3172\n",
      "Epoch [57/200], Train loss: 0.1834, Validation loss 0.2483\n",
      "Epoch [58/200], Train loss: 0.1506, Validation loss 0.2204\n",
      "Epoch [59/200], Train loss: 0.1928, Validation loss 0.3056\n",
      "Epoch [60/200], Train loss: 0.2129, Validation loss 0.2640\n",
      "Epoch [61/200], Train loss: 0.4906, Validation loss 0.2489\n",
      "Epoch [62/200], Train loss: 0.1830, Validation loss 0.2322\n",
      "Epoch [63/200], Train loss: 0.5969, Validation loss 0.3086\n",
      "Epoch [64/200], Train loss: 0.3061, Validation loss 0.3221\n",
      "Epoch [65/200], Train loss: 0.2550, Validation loss 0.3036\n",
      "Epoch [66/200], Train loss: 0.3240, Validation loss 0.2436\n",
      "Epoch [67/200], Train loss: 0.1693, Validation loss 0.2593\n",
      "Epoch [68/200], Train loss: 0.1586, Validation loss 0.2942\n",
      "Epoch [69/200], Train loss: 0.5637, Validation loss 0.2891\n",
      "Epoch [70/200], Train loss: 0.1769, Validation loss 0.2534\n",
      "Epoch [71/200], Train loss: 0.1201, Validation loss 0.3302\n",
      "Epoch [72/200], Train loss: 0.5999, Validation loss 0.2348\n",
      "Epoch [73/200], Train loss: 0.3545, Validation loss 0.3099\n",
      "Epoch [74/200], Train loss: 0.1997, Validation loss 0.2937\n",
      "Epoch [75/200], Train loss: 0.2402, Validation loss 0.2141\n",
      "Epoch [76/200], Train loss: 0.1250, Validation loss 0.2343\n",
      "Epoch [77/200], Train loss: 0.1345, Validation loss 0.2838\n",
      "Epoch [78/200], Train loss: 0.1092, Validation loss 0.3560\n",
      "Epoch [79/200], Train loss: 0.1632, Validation loss 0.2648\n",
      "Epoch [80/200], Train loss: 0.0976, Validation loss 0.2285\n",
      "Epoch [81/200], Train loss: 0.2162, Validation loss 0.2807\n",
      "Epoch [82/200], Train loss: 0.1516, Validation loss 0.2634\n",
      "Epoch [83/200], Train loss: 0.0881, Validation loss 0.2522\n",
      "Epoch [84/200], Train loss: 0.2106, Validation loss 0.2732\n",
      "Epoch [85/200], Train loss: 0.1440, Validation loss 0.2481\n",
      "Epoch [86/200], Train loss: 0.0952, Validation loss 0.2805\n",
      "Epoch [87/200], Train loss: 0.6670, Validation loss 0.3849\n",
      "Epoch [88/200], Train loss: 0.6161, Validation loss 0.2499\n",
      "Epoch [89/200], Train loss: 0.0921, Validation loss 0.3169\n",
      "Epoch [90/200], Train loss: 0.6667, Validation loss 0.2782\n",
      "Epoch [91/200], Train loss: 0.4231, Validation loss 0.3175\n",
      "Epoch [92/200], Train loss: 0.1153, Validation loss 0.2622\n",
      "Epoch [93/200], Train loss: 0.2248, Validation loss 0.3173\n",
      "Epoch [94/200], Train loss: 0.2963, Validation loss 0.2026\n",
      "Epoch [95/200], Train loss: 0.5184, Validation loss 0.2397\n",
      "Epoch [96/200], Train loss: 0.1293, Validation loss 0.2492\n",
      "Epoch [97/200], Train loss: 0.1198, Validation loss 0.2249\n",
      "Epoch [98/200], Train loss: 0.0622, Validation loss 0.3135\n",
      "Epoch [99/200], Train loss: 0.1941, Validation loss 0.2196\n",
      "Epoch [100/200], Train loss: 0.3221, Validation loss 0.2897\n",
      "Epoch [101/200], Train loss: 0.4137, Validation loss 0.2437\n",
      "Epoch [102/200], Train loss: 0.1229, Validation loss 0.2269\n",
      "Epoch [103/200], Train loss: 0.0906, Validation loss 0.2350\n",
      "Epoch [104/200], Train loss: 0.1248, Validation loss 0.2034\n",
      "Epoch [105/200], Train loss: 0.1306, Validation loss 0.2055\n",
      "Epoch [106/200], Train loss: 0.3742, Validation loss 0.2365\n",
      "Epoch [107/200], Train loss: 1.0868, Validation loss 0.3276\n",
      "Epoch [108/200], Train loss: 0.1852, Validation loss 0.2887\n",
      "Epoch [109/200], Train loss: 0.0576, Validation loss 0.2684\n",
      "Epoch [110/200], Train loss: 0.6366, Validation loss 0.2602\n",
      "Epoch [111/200], Train loss: 0.6913, Validation loss 0.2612\n",
      "Epoch [112/200], Train loss: 0.2190, Validation loss 0.2548\n",
      "Epoch [113/200], Train loss: 0.0541, Validation loss 0.3098\n",
      "Training finished! after 113 epochs, Model saved to ./model-fold-6.pth\n",
      "Epoch [1/200], Train loss: 1.0956, Validation loss 1.0454\n",
      "Epoch [2/200], Train loss: 0.8072, Validation loss 0.9462\n",
      "Epoch [3/200], Train loss: 0.9377, Validation loss 0.8942\n",
      "Epoch [4/200], Train loss: 0.6244, Validation loss 0.8434\n",
      "Epoch [5/200], Train loss: 0.8537, Validation loss 0.8496\n",
      "Epoch [6/200], Train loss: 0.7709, Validation loss 0.7986\n",
      "Epoch [7/200], Train loss: 0.8311, Validation loss 0.7366\n",
      "Epoch [8/200], Train loss: 0.6652, Validation loss 0.7365\n",
      "Epoch [9/200], Train loss: 0.8485, Validation loss 0.6944\n",
      "Epoch [10/200], Train loss: 0.7528, Validation loss 0.6693\n",
      "Epoch [11/200], Train loss: 0.7892, Validation loss 0.6715\n",
      "Epoch [12/200], Train loss: 0.6134, Validation loss 0.5952\n",
      "Epoch [13/200], Train loss: 0.5478, Validation loss 0.5850\n",
      "Epoch [14/200], Train loss: 0.7576, Validation loss 0.5949\n",
      "Epoch [15/200], Train loss: 0.4937, Validation loss 0.5941\n",
      "Epoch [16/200], Train loss: 0.4668, Validation loss 0.5429\n",
      "Epoch [17/200], Train loss: 0.5189, Validation loss 0.5711\n",
      "Epoch [18/200], Train loss: 0.4120, Validation loss 0.4918\n",
      "Epoch [19/200], Train loss: 0.6561, Validation loss 0.5117\n",
      "Epoch [20/200], Train loss: 0.4010, Validation loss 0.5123\n",
      "Epoch [21/200], Train loss: 0.5285, Validation loss 0.5202\n",
      "Epoch [22/200], Train loss: 0.3032, Validation loss 0.4282\n",
      "Epoch [23/200], Train loss: 0.4901, Validation loss 0.3852\n",
      "Epoch [24/200], Train loss: 0.4833, Validation loss 0.4049\n",
      "Epoch [25/200], Train loss: 0.4637, Validation loss 0.4692\n",
      "Epoch [26/200], Train loss: 0.2536, Validation loss 0.4067\n",
      "Epoch [27/200], Train loss: 0.4422, Validation loss 0.3410\n",
      "Epoch [28/200], Train loss: 0.4040, Validation loss 0.4312\n",
      "Epoch [29/200], Train loss: 0.2816, Validation loss 0.3211\n",
      "Epoch [30/200], Train loss: 0.2109, Validation loss 0.3205\n",
      "Epoch [31/200], Train loss: 0.3231, Validation loss 0.3505\n",
      "Epoch [32/200], Train loss: 0.2545, Validation loss 0.3163\n",
      "Epoch [33/200], Train loss: 0.2969, Validation loss 0.2917\n",
      "Epoch [34/200], Train loss: 0.2982, Validation loss 0.3050\n",
      "Epoch [35/200], Train loss: 0.3869, Validation loss 0.2802\n",
      "Epoch [36/200], Train loss: 0.3175, Validation loss 0.2542\n",
      "Epoch [37/200], Train loss: 0.3773, Validation loss 0.3046\n",
      "Epoch [38/200], Train loss: 0.3142, Validation loss 0.2671\n",
      "Epoch [39/200], Train loss: 0.2890, Validation loss 0.2490\n",
      "Epoch [40/200], Train loss: 0.1524, Validation loss 0.2538\n",
      "Epoch [41/200], Train loss: 0.2899, Validation loss 0.2730\n",
      "Epoch [42/200], Train loss: 0.1946, Validation loss 0.2681\n",
      "Epoch [43/200], Train loss: 0.3017, Validation loss 0.2225\n",
      "Epoch [44/200], Train loss: 0.1971, Validation loss 0.2224\n",
      "Epoch [45/200], Train loss: 0.2970, Validation loss 0.2530\n",
      "Epoch [46/200], Train loss: 0.2185, Validation loss 0.2536\n",
      "Epoch [47/200], Train loss: 0.1568, Validation loss 0.2117\n",
      "Epoch [48/200], Train loss: 0.2704, Validation loss 0.2250\n",
      "Epoch [49/200], Train loss: 0.2288, Validation loss 0.2139\n",
      "Epoch [50/200], Train loss: 0.2780, Validation loss 0.2077\n",
      "Epoch [51/200], Train loss: 0.2354, Validation loss 0.2281\n",
      "Epoch [52/200], Train loss: 0.3077, Validation loss 0.2169\n",
      "Epoch [53/200], Train loss: 0.1563, Validation loss 0.2129\n",
      "Epoch [54/200], Train loss: 0.2003, Validation loss 0.2109\n",
      "Epoch [55/200], Train loss: 0.2071, Validation loss 0.1873\n",
      "Epoch [56/200], Train loss: 0.1421, Validation loss 0.1917\n",
      "Epoch [57/200], Train loss: 0.2604, Validation loss 0.2083\n",
      "Epoch [58/200], Train loss: 0.1830, Validation loss 0.2236\n",
      "Epoch [59/200], Train loss: 0.1171, Validation loss 0.1787\n",
      "Epoch [60/200], Train loss: 0.1280, Validation loss 0.2170\n",
      "Epoch [61/200], Train loss: 0.2354, Validation loss 0.2090\n",
      "Epoch [62/200], Train loss: 0.2073, Validation loss 0.2024\n",
      "Epoch [63/200], Train loss: 0.1744, Validation loss 0.2157\n",
      "Epoch [64/200], Train loss: 0.1159, Validation loss 0.1748\n",
      "Epoch [65/200], Train loss: 0.2575, Validation loss 0.1931\n",
      "Epoch [66/200], Train loss: 0.2626, Validation loss 0.1986\n",
      "Epoch [67/200], Train loss: 0.1728, Validation loss 0.1783\n",
      "Epoch [68/200], Train loss: 0.1931, Validation loss 0.2025\n",
      "Epoch [69/200], Train loss: 0.2353, Validation loss 0.1938\n",
      "Epoch [70/200], Train loss: 0.1436, Validation loss 0.1752\n",
      "Epoch [71/200], Train loss: 0.2209, Validation loss 0.2039\n",
      "Epoch [72/200], Train loss: 0.3203, Validation loss 0.2141\n",
      "Epoch [73/200], Train loss: 0.1977, Validation loss 0.1757\n",
      "Epoch [74/200], Train loss: 0.2020, Validation loss 0.1697\n",
      "Epoch [75/200], Train loss: 0.1288, Validation loss 0.1718\n",
      "Epoch [76/200], Train loss: 0.2317, Validation loss 0.1844\n",
      "Epoch [77/200], Train loss: 0.2307, Validation loss 0.2705\n",
      "Epoch [78/200], Train loss: 0.3950, Validation loss 0.2070\n",
      "Epoch [79/200], Train loss: 0.2526, Validation loss 0.1955\n",
      "Epoch [80/200], Train loss: 0.1718, Validation loss 0.1530\n",
      "Epoch [81/200], Train loss: 0.1269, Validation loss 0.1672\n",
      "Epoch [82/200], Train loss: 0.2341, Validation loss 0.1760\n",
      "Epoch [83/200], Train loss: 0.0611, Validation loss 0.1458\n",
      "Epoch [84/200], Train loss: 0.2112, Validation loss 0.1592\n",
      "Epoch [85/200], Train loss: 0.1735, Validation loss 0.1740\n",
      "Epoch [86/200], Train loss: 0.1759, Validation loss 0.1598\n",
      "Epoch [87/200], Train loss: 0.1657, Validation loss 0.1610\n",
      "Epoch [88/200], Train loss: 0.1055, Validation loss 0.1568\n",
      "Epoch [89/200], Train loss: 0.2080, Validation loss 0.1594\n",
      "Epoch [90/200], Train loss: 0.3008, Validation loss 0.2135\n",
      "Epoch [91/200], Train loss: 0.2024, Validation loss 0.1640\n",
      "Epoch [92/200], Train loss: 0.0944, Validation loss 0.1463\n",
      "Epoch [93/200], Train loss: 0.1654, Validation loss 0.1552\n",
      "Epoch [94/200], Train loss: 0.1344, Validation loss 0.1456\n",
      "Epoch [95/200], Train loss: 0.1454, Validation loss 0.1806\n",
      "Epoch [96/200], Train loss: 0.1112, Validation loss 0.1549\n",
      "Epoch [97/200], Train loss: 0.1068, Validation loss 0.1426\n",
      "Epoch [98/200], Train loss: 0.1438, Validation loss 0.1447\n",
      "Epoch [99/200], Train loss: 0.0961, Validation loss 0.1744\n",
      "Epoch [100/200], Train loss: 0.1914, Validation loss 0.1461\n",
      "Epoch [101/200], Train loss: 0.1312, Validation loss 0.1505\n",
      "Epoch [102/200], Train loss: 0.1315, Validation loss 0.1456\n",
      "Epoch [103/200], Train loss: 0.1098, Validation loss 0.1673\n",
      "Epoch [104/200], Train loss: 0.1007, Validation loss 0.1503\n",
      "Epoch [105/200], Train loss: 0.1087, Validation loss 0.1471\n",
      "Epoch [106/200], Train loss: 0.0618, Validation loss 0.1474\n",
      "Epoch [107/200], Train loss: 0.1255, Validation loss 0.1396\n",
      "Epoch [108/200], Train loss: 0.1143, Validation loss 0.1704\n",
      "Epoch [109/200], Train loss: 0.1209, Validation loss 0.1407\n",
      "Epoch [110/200], Train loss: 0.1937, Validation loss 0.1393\n",
      "Epoch [111/200], Train loss: 0.0586, Validation loss 0.1843\n",
      "Epoch [112/200], Train loss: 0.1996, Validation loss 0.1385\n",
      "Epoch [113/200], Train loss: 0.2088, Validation loss 0.1660\n",
      "Epoch [114/200], Train loss: 0.1177, Validation loss 0.1435\n",
      "Epoch [115/200], Train loss: 0.1683, Validation loss 0.1445\n",
      "Epoch [116/200], Train loss: 0.1656, Validation loss 0.1369\n",
      "Epoch [117/200], Train loss: 0.1018, Validation loss 0.1469\n",
      "Epoch [118/200], Train loss: 0.1322, Validation loss 0.1349\n",
      "Epoch [119/200], Train loss: 0.1069, Validation loss 0.1333\n",
      "Epoch [120/200], Train loss: 0.1903, Validation loss 0.1397\n",
      "Epoch [121/200], Train loss: 0.2137, Validation loss 0.1559\n",
      "Epoch [122/200], Train loss: 0.1509, Validation loss 0.1307\n",
      "Epoch [123/200], Train loss: 0.0576, Validation loss 0.1377\n",
      "Epoch [124/200], Train loss: 0.1103, Validation loss 0.1423\n",
      "Epoch [125/200], Train loss: 0.1183, Validation loss 0.1368\n",
      "Epoch [126/200], Train loss: 0.1310, Validation loss 0.1317\n",
      "Epoch [127/200], Train loss: 0.1150, Validation loss 0.1388\n",
      "Epoch [128/200], Train loss: 0.1296, Validation loss 0.1294\n",
      "Epoch [129/200], Train loss: 0.1397, Validation loss 0.1537\n",
      "Epoch [130/200], Train loss: 0.1372, Validation loss 0.1359\n",
      "Epoch [131/200], Train loss: 0.0539, Validation loss 0.1483\n",
      "Epoch [132/200], Train loss: 0.1329, Validation loss 0.1288\n",
      "Epoch [133/200], Train loss: 0.1294, Validation loss 0.1521\n",
      "Epoch [134/200], Train loss: 0.1105, Validation loss 0.1351\n",
      "Epoch [135/200], Train loss: 0.1214, Validation loss 0.1496\n",
      "Epoch [136/200], Train loss: 0.1359, Validation loss 0.1239\n",
      "Epoch [137/200], Train loss: 0.1803, Validation loss 0.1418\n",
      "Epoch [138/200], Train loss: 0.0467, Validation loss 0.1258\n",
      "Epoch [139/200], Train loss: 0.1228, Validation loss 0.1329\n",
      "Epoch [140/200], Train loss: 0.0852, Validation loss 0.1291\n",
      "Epoch [141/200], Train loss: 0.1173, Validation loss 0.1224\n",
      "Epoch [142/200], Train loss: 0.0981, Validation loss 0.1305\n",
      "Epoch [143/200], Train loss: 0.1777, Validation loss 0.1250\n",
      "Epoch [144/200], Train loss: 0.1255, Validation loss 0.1313\n",
      "Epoch [145/200], Train loss: 0.0789, Validation loss 0.1326\n",
      "Epoch [146/200], Train loss: 0.1697, Validation loss 0.1345\n",
      "Epoch [147/200], Train loss: 0.1730, Validation loss 0.1258\n",
      "Epoch [148/200], Train loss: 0.0347, Validation loss 0.1309\n",
      "Epoch [149/200], Train loss: 0.1865, Validation loss 0.1303\n",
      "Epoch [150/200], Train loss: 0.2509, Validation loss 0.1291\n",
      "Epoch [151/200], Train loss: 0.1660, Validation loss 0.1412\n",
      "Epoch [152/200], Train loss: 0.1078, Validation loss 0.1215\n",
      "Epoch [153/200], Train loss: 0.2327, Validation loss 0.1473\n",
      "Epoch [154/200], Train loss: 0.1537, Validation loss 0.1337\n",
      "Epoch [155/200], Train loss: 0.0794, Validation loss 0.1258\n",
      "Epoch [156/200], Train loss: 0.1450, Validation loss 0.1330\n",
      "Epoch [157/200], Train loss: 0.0390, Validation loss 0.1381\n",
      "Epoch [158/200], Train loss: 0.0414, Validation loss 0.1275\n",
      "Epoch [159/200], Train loss: 0.3021, Validation loss 0.1803\n",
      "Epoch [160/200], Train loss: 0.1099, Validation loss 0.1215\n",
      "Epoch [161/200], Train loss: 0.1254, Validation loss 0.1315\n",
      "Epoch [162/200], Train loss: 0.1580, Validation loss 0.1364\n",
      "Epoch [163/200], Train loss: 0.1380, Validation loss 0.1310\n",
      "Epoch [164/200], Train loss: 0.1152, Validation loss 0.1278\n",
      "Epoch [165/200], Train loss: 0.0757, Validation loss 0.1392\n",
      "Epoch [166/200], Train loss: 0.0791, Validation loss 0.1279\n",
      "Epoch [167/200], Train loss: 0.1192, Validation loss 0.1368\n",
      "Epoch [168/200], Train loss: 0.1237, Validation loss 0.1993\n",
      "Epoch [169/200], Train loss: 0.1318, Validation loss 0.1191\n",
      "Epoch [170/200], Train loss: 0.1297, Validation loss 0.1248\n",
      "Epoch [171/200], Train loss: 0.1133, Validation loss 0.1281\n",
      "Epoch [172/200], Train loss: 0.0947, Validation loss 0.1231\n",
      "Epoch [173/200], Train loss: 0.1356, Validation loss 0.1217\n",
      "Epoch [174/200], Train loss: 0.1726, Validation loss 0.1257\n",
      "Epoch [175/200], Train loss: 0.0966, Validation loss 0.1156\n",
      "Epoch [176/200], Train loss: 0.1168, Validation loss 0.1302\n",
      "Epoch [177/200], Train loss: 0.1075, Validation loss 0.1379\n",
      "Epoch [178/200], Train loss: 0.2139, Validation loss 0.1695\n",
      "Epoch [179/200], Train loss: 0.1118, Validation loss 0.1203\n",
      "Epoch [180/200], Train loss: 0.1537, Validation loss 0.1229\n",
      "Epoch [181/200], Train loss: 0.1881, Validation loss 0.1195\n",
      "Epoch [182/200], Train loss: 0.1320, Validation loss 0.1258\n",
      "Epoch [183/200], Train loss: 0.1100, Validation loss 0.1255\n",
      "Epoch [184/200], Train loss: 0.1129, Validation loss 0.1370\n",
      "Epoch [185/200], Train loss: 0.1021, Validation loss 0.1218\n",
      "Epoch [186/200], Train loss: 0.0732, Validation loss 0.1299\n",
      "Epoch [187/200], Train loss: 0.1496, Validation loss 0.1291\n",
      "Epoch [188/200], Train loss: 0.1269, Validation loss 0.1254\n",
      "Epoch [189/200], Train loss: 0.0937, Validation loss 0.1287\n",
      "Epoch [190/200], Train loss: 0.0794, Validation loss 0.1268\n",
      "Epoch [191/200], Train loss: 0.0847, Validation loss 0.1227\n",
      "Epoch [192/200], Train loss: 0.1225, Validation loss 0.1248\n",
      "Epoch [193/200], Train loss: 0.1535, Validation loss 0.1219\n",
      "Epoch [194/200], Train loss: 0.0515, Validation loss 0.1277\n",
      "Training finished! after 194 epochs, Model saved to ./model-fold-7.pth\n",
      "Epoch [1/200], Train loss: 1.0951, Validation loss 1.1925\n",
      "Epoch [2/200], Train loss: 1.0557, Validation loss 1.0930\n",
      "Epoch [3/200], Train loss: 0.9735, Validation loss 0.9694\n",
      "Epoch [4/200], Train loss: 1.0932, Validation loss 0.9757\n",
      "Epoch [5/200], Train loss: 0.7981, Validation loss 0.8891\n",
      "Epoch [6/200], Train loss: 0.9317, Validation loss 0.8263\n",
      "Epoch [7/200], Train loss: 0.7651, Validation loss 0.7747\n",
      "Epoch [8/200], Train loss: 0.7327, Validation loss 0.7470\n",
      "Epoch [9/200], Train loss: 0.5734, Validation loss 0.6708\n",
      "Epoch [10/200], Train loss: 0.6029, Validation loss 0.6526\n",
      "Epoch [11/200], Train loss: 0.6331, Validation loss 0.7026\n",
      "Epoch [12/200], Train loss: 0.6261, Validation loss 0.6638\n",
      "Epoch [13/200], Train loss: 0.5297, Validation loss 0.6290\n",
      "Epoch [14/200], Train loss: 0.5668, Validation loss 0.5533\n",
      "Epoch [15/200], Train loss: 0.5219, Validation loss 0.5524\n",
      "Epoch [16/200], Train loss: 0.4771, Validation loss 0.4695\n",
      "Epoch [17/200], Train loss: 0.5642, Validation loss 0.5022\n",
      "Epoch [18/200], Train loss: 0.4018, Validation loss 0.4487\n",
      "Epoch [19/200], Train loss: 0.3819, Validation loss 0.4196\n",
      "Epoch [20/200], Train loss: 0.4912, Validation loss 0.4863\n",
      "Epoch [21/200], Train loss: 0.4228, Validation loss 0.3799\n",
      "Epoch [22/200], Train loss: 0.3924, Validation loss 0.3858\n",
      "Epoch [23/200], Train loss: 0.4008, Validation loss 0.3710\n",
      "Epoch [24/200], Train loss: 0.3757, Validation loss 0.3369\n",
      "Epoch [25/200], Train loss: 0.3515, Validation loss 0.3344\n",
      "Epoch [26/200], Train loss: 0.3298, Validation loss 0.3082\n",
      "Epoch [27/200], Train loss: 0.2353, Validation loss 0.3022\n",
      "Epoch [28/200], Train loss: 0.2548, Validation loss 0.2856\n",
      "Epoch [29/200], Train loss: 0.2847, Validation loss 0.3135\n",
      "Epoch [30/200], Train loss: 0.2211, Validation loss 0.2695\n",
      "Epoch [31/200], Train loss: 0.2395, Validation loss 0.2633\n",
      "Epoch [32/200], Train loss: 0.2187, Validation loss 0.2690\n",
      "Epoch [33/200], Train loss: 0.3414, Validation loss 0.2757\n",
      "Epoch [34/200], Train loss: 0.1910, Validation loss 0.2559\n",
      "Epoch [35/200], Train loss: 0.2756, Validation loss 0.2529\n",
      "Epoch [36/200], Train loss: 0.2265, Validation loss 0.2329\n",
      "Epoch [37/200], Train loss: 0.3374, Validation loss 0.2476\n",
      "Epoch [38/200], Train loss: 0.2678, Validation loss 0.2318\n",
      "Epoch [39/200], Train loss: 0.3453, Validation loss 0.2251\n",
      "Epoch [40/200], Train loss: 0.2004, Validation loss 0.2242\n",
      "Epoch [41/200], Train loss: 0.3137, Validation loss 0.2313\n",
      "Epoch [42/200], Train loss: 0.1643, Validation loss 0.2230\n",
      "Epoch [43/200], Train loss: 0.1886, Validation loss 0.2081\n",
      "Epoch [44/200], Train loss: 0.2340, Validation loss 0.2128\n",
      "Epoch [45/200], Train loss: 0.1463, Validation loss 0.2022\n",
      "Epoch [46/200], Train loss: 0.2043, Validation loss 0.2040\n",
      "Epoch [47/200], Train loss: 0.1652, Validation loss 0.2046\n",
      "Epoch [48/200], Train loss: 0.1900, Validation loss 0.2005\n",
      "Epoch [49/200], Train loss: 0.1612, Validation loss 0.2181\n",
      "Epoch [50/200], Train loss: 0.2046, Validation loss 0.1933\n",
      "Epoch [51/200], Train loss: 0.1291, Validation loss 0.1978\n",
      "Epoch [52/200], Train loss: 0.1465, Validation loss 0.1904\n",
      "Epoch [53/200], Train loss: 0.3227, Validation loss 0.1723\n",
      "Epoch [54/200], Train loss: 0.2132, Validation loss 0.2254\n",
      "Epoch [55/200], Train loss: 0.1750, Validation loss 0.1723\n",
      "Epoch [56/200], Train loss: 0.2364, Validation loss 0.1824\n",
      "Epoch [57/200], Train loss: 0.2853, Validation loss 0.1720\n",
      "Epoch [58/200], Train loss: 0.2103, Validation loss 0.1727\n",
      "Epoch [59/200], Train loss: 0.1522, Validation loss 0.1662\n",
      "Epoch [60/200], Train loss: 0.1700, Validation loss 0.1625\n",
      "Epoch [61/200], Train loss: 0.1473, Validation loss 0.1798\n",
      "Epoch [62/200], Train loss: 0.1943, Validation loss 0.1839\n",
      "Epoch [63/200], Train loss: 0.1571, Validation loss 0.1814\n",
      "Epoch [64/200], Train loss: 0.2253, Validation loss 0.1671\n",
      "Epoch [65/200], Train loss: 0.0525, Validation loss 0.1919\n",
      "Epoch [66/200], Train loss: 0.1440, Validation loss 0.1880\n",
      "Epoch [67/200], Train loss: 0.1824, Validation loss 0.1678\n",
      "Epoch [68/200], Train loss: 0.1262, Validation loss 0.1646\n",
      "Epoch [69/200], Train loss: 0.1436, Validation loss 0.1668\n",
      "Epoch [70/200], Train loss: 0.1721, Validation loss 0.1996\n",
      "Epoch [71/200], Train loss: 0.2652, Validation loss 0.1668\n",
      "Epoch [72/200], Train loss: 0.1972, Validation loss 0.1648\n",
      "Epoch [73/200], Train loss: 0.1650, Validation loss 0.1508\n",
      "Epoch [74/200], Train loss: 0.1072, Validation loss 0.1684\n",
      "Epoch [75/200], Train loss: 0.0822, Validation loss 0.2082\n",
      "Epoch [76/200], Train loss: 0.1719, Validation loss 0.1658\n",
      "Epoch [77/200], Train loss: 0.1682, Validation loss 0.1567\n",
      "Epoch [78/200], Train loss: 0.1205, Validation loss 0.1609\n",
      "Epoch [79/200], Train loss: 0.1487, Validation loss 0.1629\n",
      "Epoch [80/200], Train loss: 0.1484, Validation loss 0.1665\n",
      "Epoch [81/200], Train loss: 0.1004, Validation loss 0.1424\n",
      "Epoch [82/200], Train loss: 0.1559, Validation loss 0.1661\n",
      "Epoch [83/200], Train loss: 0.1276, Validation loss 0.1446\n",
      "Epoch [84/200], Train loss: 0.1375, Validation loss 0.1908\n",
      "Epoch [85/200], Train loss: 0.1474, Validation loss 0.1471\n",
      "Epoch [86/200], Train loss: 0.1414, Validation loss 0.1638\n",
      "Epoch [87/200], Train loss: 0.1646, Validation loss 0.1798\n",
      "Epoch [88/200], Train loss: 0.1756, Validation loss 0.2004\n",
      "Epoch [89/200], Train loss: 0.1718, Validation loss 0.1448\n",
      "Epoch [90/200], Train loss: 0.1575, Validation loss 0.1579\n",
      "Epoch [91/200], Train loss: 0.0788, Validation loss 0.1411\n",
      "Epoch [92/200], Train loss: 0.1213, Validation loss 0.1531\n",
      "Epoch [93/200], Train loss: 0.1281, Validation loss 0.1487\n",
      "Epoch [94/200], Train loss: 0.1313, Validation loss 0.1528\n",
      "Epoch [95/200], Train loss: 0.2485, Validation loss 0.1526\n",
      "Epoch [96/200], Train loss: 0.2361, Validation loss 0.1396\n",
      "Epoch [97/200], Train loss: 0.1460, Validation loss 0.1606\n",
      "Epoch [98/200], Train loss: 0.3902, Validation loss 0.1694\n",
      "Epoch [99/200], Train loss: 0.0663, Validation loss 0.1366\n",
      "Epoch [100/200], Train loss: 0.1490, Validation loss 0.1478\n",
      "Epoch [101/200], Train loss: 0.0768, Validation loss 0.1402\n",
      "Epoch [102/200], Train loss: 0.1850, Validation loss 0.1613\n",
      "Epoch [103/200], Train loss: 0.4553, Validation loss 0.1396\n",
      "Epoch [104/200], Train loss: 0.0955, Validation loss 0.1566\n",
      "Epoch [105/200], Train loss: 0.1107, Validation loss 0.1476\n",
      "Epoch [106/200], Train loss: 0.1130, Validation loss 0.1417\n",
      "Epoch [107/200], Train loss: 0.4061, Validation loss 0.2293\n",
      "Epoch [108/200], Train loss: 0.1345, Validation loss 0.1397\n",
      "Epoch [109/200], Train loss: 0.3102, Validation loss 0.1640\n",
      "Epoch [110/200], Train loss: 0.1667, Validation loss 0.1412\n",
      "Epoch [111/200], Train loss: 0.0606, Validation loss 0.1532\n",
      "Epoch [112/200], Train loss: 0.1418, Validation loss 0.1209\n",
      "Epoch [113/200], Train loss: 0.1580, Validation loss 0.1564\n",
      "Epoch [114/200], Train loss: 0.2393, Validation loss 0.1634\n",
      "Epoch [115/200], Train loss: 0.1507, Validation loss 0.1380\n",
      "Epoch [116/200], Train loss: 0.1113, Validation loss 0.1581\n",
      "Epoch [117/200], Train loss: 0.0695, Validation loss 0.1505\n",
      "Epoch [118/200], Train loss: 0.1504, Validation loss 0.1373\n",
      "Epoch [119/200], Train loss: 0.1831, Validation loss 0.1475\n",
      "Epoch [120/200], Train loss: 0.1281, Validation loss 0.1579\n",
      "Epoch [121/200], Train loss: 0.1107, Validation loss 0.1319\n",
      "Epoch [122/200], Train loss: 0.1477, Validation loss 0.1424\n",
      "Epoch [123/200], Train loss: 0.2156, Validation loss 0.1438\n",
      "Epoch [124/200], Train loss: 0.1366, Validation loss 0.1358\n",
      "Epoch [125/200], Train loss: 0.0877, Validation loss 0.1371\n",
      "Epoch [126/200], Train loss: 0.0881, Validation loss 0.1298\n",
      "Epoch [127/200], Train loss: 0.1282, Validation loss 0.1334\n",
      "Epoch [128/200], Train loss: 0.1585, Validation loss 0.1640\n",
      "Epoch [129/200], Train loss: 0.1272, Validation loss 0.1392\n",
      "Epoch [130/200], Train loss: 0.1876, Validation loss 0.1430\n",
      "Epoch [131/200], Train loss: 0.1759, Validation loss 0.1491\n",
      "Training finished! after 131 epochs, Model saved to ./model-fold-8.pth\n",
      "Epoch [1/200], Train loss: 1.1890, Validation loss 1.2202\n",
      "Epoch [2/200], Train loss: 1.2770, Validation loss 1.1780\n",
      "Epoch [3/200], Train loss: 1.1935, Validation loss 1.0495\n",
      "Epoch [4/200], Train loss: 1.0135, Validation loss 0.9278\n",
      "Epoch [5/200], Train loss: 1.0916, Validation loss 0.9383\n",
      "Epoch [6/200], Train loss: 0.8730, Validation loss 0.8854\n",
      "Epoch [7/200], Train loss: 0.6608, Validation loss 0.8107\n",
      "Epoch [8/200], Train loss: 0.7513, Validation loss 0.7849\n",
      "Epoch [9/200], Train loss: 0.8933, Validation loss 0.9342\n",
      "Epoch [10/200], Train loss: 0.6434, Validation loss 0.7705\n",
      "Epoch [11/200], Train loss: 0.6639, Validation loss 0.7082\n",
      "Epoch [12/200], Train loss: 0.6029, Validation loss 0.6401\n",
      "Epoch [13/200], Train loss: 0.6531, Validation loss 0.5957\n",
      "Epoch [14/200], Train loss: 0.7469, Validation loss 0.6454\n",
      "Epoch [15/200], Train loss: 0.4999, Validation loss 0.5607\n",
      "Epoch [16/200], Train loss: 0.5273, Validation loss 0.5310\n",
      "Epoch [17/200], Train loss: 0.4515, Validation loss 0.4878\n",
      "Epoch [18/200], Train loss: 0.5739, Validation loss 0.4690\n",
      "Epoch [19/200], Train loss: 0.4161, Validation loss 0.4592\n",
      "Epoch [20/200], Train loss: 0.4379, Validation loss 0.4275\n",
      "Epoch [21/200], Train loss: 0.3009, Validation loss 0.4152\n",
      "Epoch [22/200], Train loss: 0.3615, Validation loss 0.3951\n",
      "Epoch [23/200], Train loss: 0.3722, Validation loss 0.4312\n",
      "Epoch [24/200], Train loss: 0.3995, Validation loss 0.4286\n",
      "Epoch [25/200], Train loss: 0.4189, Validation loss 0.3842\n",
      "Epoch [26/200], Train loss: 0.2626, Validation loss 0.3572\n",
      "Epoch [27/200], Train loss: 0.3916, Validation loss 0.3699\n",
      "Epoch [28/200], Train loss: 0.5033, Validation loss 0.3851\n",
      "Epoch [29/200], Train loss: 0.3129, Validation loss 0.3164\n",
      "Epoch [30/200], Train loss: 0.3197, Validation loss 0.3156\n",
      "Epoch [31/200], Train loss: 0.3316, Validation loss 0.3318\n",
      "Epoch [32/200], Train loss: 0.3293, Validation loss 0.3232\n",
      "Epoch [33/200], Train loss: 0.2550, Validation loss 0.3246\n",
      "Epoch [34/200], Train loss: 0.3344, Validation loss 0.2907\n",
      "Epoch [35/200], Train loss: 0.4542, Validation loss 0.3048\n",
      "Epoch [36/200], Train loss: 0.3287, Validation loss 0.2978\n",
      "Epoch [37/200], Train loss: 0.2832, Validation loss 0.3274\n",
      "Epoch [38/200], Train loss: 0.2898, Validation loss 0.2772\n",
      "Epoch [39/200], Train loss: 0.3815, Validation loss 0.2699\n",
      "Epoch [40/200], Train loss: 0.2772, Validation loss 0.2572\n",
      "Epoch [41/200], Train loss: 0.1565, Validation loss 0.2650\n",
      "Epoch [42/200], Train loss: 0.1710, Validation loss 0.2460\n",
      "Epoch [43/200], Train loss: 0.3184, Validation loss 0.3063\n",
      "Epoch [44/200], Train loss: 0.2694, Validation loss 0.2399\n",
      "Epoch [45/200], Train loss: 0.2682, Validation loss 0.2539\n",
      "Epoch [46/200], Train loss: 0.2253, Validation loss 0.2464\n",
      "Epoch [47/200], Train loss: 0.1696, Validation loss 0.2349\n",
      "Epoch [48/200], Train loss: 0.2379, Validation loss 0.2459\n",
      "Epoch [49/200], Train loss: 0.1734, Validation loss 0.2331\n",
      "Epoch [50/200], Train loss: 0.2692, Validation loss 0.2744\n",
      "Epoch [51/200], Train loss: 0.2581, Validation loss 0.2475\n",
      "Epoch [52/200], Train loss: 0.3196, Validation loss 0.2338\n",
      "Epoch [53/200], Train loss: 0.2896, Validation loss 0.2714\n",
      "Epoch [54/200], Train loss: 0.1626, Validation loss 0.2152\n",
      "Epoch [55/200], Train loss: 0.2224, Validation loss 0.2209\n",
      "Epoch [56/200], Train loss: 0.3083, Validation loss 0.2534\n",
      "Epoch [57/200], Train loss: 0.1227, Validation loss 0.2234\n",
      "Epoch [58/200], Train loss: 0.2956, Validation loss 0.2486\n",
      "Epoch [59/200], Train loss: 0.1557, Validation loss 0.2215\n",
      "Epoch [60/200], Train loss: 0.1699, Validation loss 0.2416\n",
      "Epoch [61/200], Train loss: 0.2752, Validation loss 0.2741\n",
      "Epoch [62/200], Train loss: 0.2457, Validation loss 0.2228\n",
      "Epoch [63/200], Train loss: 0.1874, Validation loss 0.2179\n",
      "Epoch [64/200], Train loss: 0.1900, Validation loss 0.2134\n",
      "Epoch [65/200], Train loss: 0.1715, Validation loss 0.2163\n",
      "Epoch [66/200], Train loss: 0.2209, Validation loss 0.2165\n",
      "Epoch [67/200], Train loss: 0.1677, Validation loss 0.2312\n",
      "Epoch [68/200], Train loss: 0.1534, Validation loss 0.2226\n",
      "Epoch [69/200], Train loss: 0.1742, Validation loss 0.2196\n",
      "Epoch [70/200], Train loss: 0.1443, Validation loss 0.2165\n",
      "Epoch [71/200], Train loss: 0.3394, Validation loss 0.2132\n",
      "Epoch [72/200], Train loss: 0.1967, Validation loss 0.1857\n",
      "Epoch [73/200], Train loss: 0.1309, Validation loss 0.1845\n",
      "Epoch [74/200], Train loss: 0.1623, Validation loss 0.2122\n",
      "Epoch [75/200], Train loss: 0.2185, Validation loss 0.2439\n",
      "Epoch [76/200], Train loss: 0.1910, Validation loss 0.1832\n",
      "Epoch [77/200], Train loss: 0.1316, Validation loss 0.1753\n",
      "Epoch [78/200], Train loss: 0.1405, Validation loss 0.1916\n",
      "Epoch [79/200], Train loss: 0.0840, Validation loss 0.2061\n",
      "Epoch [80/200], Train loss: 0.1560, Validation loss 0.1743\n",
      "Epoch [81/200], Train loss: 0.1900, Validation loss 0.1811\n",
      "Epoch [82/200], Train loss: 0.1507, Validation loss 0.1861\n",
      "Epoch [83/200], Train loss: 0.1789, Validation loss 0.2060\n",
      "Epoch [84/200], Train loss: 0.1046, Validation loss 0.1910\n",
      "Epoch [85/200], Train loss: 0.0825, Validation loss 0.1959\n",
      "Epoch [86/200], Train loss: 0.2267, Validation loss 0.1659\n",
      "Epoch [87/200], Train loss: 0.1265, Validation loss 0.1949\n",
      "Epoch [88/200], Train loss: 0.1483, Validation loss 0.1842\n",
      "Epoch [89/200], Train loss: 0.1185, Validation loss 0.1789\n",
      "Epoch [90/200], Train loss: 0.1255, Validation loss 0.1953\n",
      "Epoch [91/200], Train loss: 0.1801, Validation loss 0.1717\n",
      "Epoch [92/200], Train loss: 0.1096, Validation loss 0.1797\n",
      "Epoch [93/200], Train loss: 0.2204, Validation loss 0.1794\n",
      "Epoch [94/200], Train loss: 0.1295, Validation loss 0.1820\n",
      "Epoch [95/200], Train loss: 0.1656, Validation loss 0.1956\n",
      "Epoch [96/200], Train loss: 0.1826, Validation loss 0.1804\n",
      "Epoch [97/200], Train loss: 0.1534, Validation loss 0.1936\n",
      "Epoch [98/200], Train loss: 0.2429, Validation loss 0.1751\n",
      "Epoch [99/200], Train loss: 0.1979, Validation loss 0.1821\n",
      "Epoch [100/200], Train loss: 0.1502, Validation loss 0.1684\n",
      "Epoch [101/200], Train loss: 0.1588, Validation loss 0.1943\n",
      "Epoch [102/200], Train loss: 0.2048, Validation loss 0.1817\n",
      "Epoch [103/200], Train loss: 0.0872, Validation loss 0.1759\n",
      "Epoch [104/200], Train loss: 0.1825, Validation loss 0.1683\n",
      "Epoch [105/200], Train loss: 0.1221, Validation loss 0.1857\n",
      "Training finished! after 105 epochs, Model saved to ./model-fold-9.pth\n",
      "Epoch [1/200], Train loss: 0.5654, Validation loss 0.6316\n",
      "Epoch [2/200], Train loss: 0.4017, Validation loss 0.3774\n",
      "Epoch [3/200], Train loss: 0.2827, Validation loss 0.2955\n",
      "Epoch [4/200], Train loss: 0.2512, Validation loss 0.2847\n",
      "Epoch [5/200], Train loss: 0.2425, Validation loss 0.2480\n",
      "Epoch [6/200], Train loss: 0.3055, Validation loss 0.2602\n",
      "Epoch [7/200], Train loss: 0.1687, Validation loss 0.2348\n",
      "Epoch [8/200], Train loss: 0.1803, Validation loss 0.1802\n",
      "Epoch [9/200], Train loss: 0.1792, Validation loss 0.1686\n",
      "Epoch [10/200], Train loss: 0.1869, Validation loss 0.1695\n",
      "Epoch [11/200], Train loss: 0.2605, Validation loss 0.2987\n",
      "Epoch [12/200], Train loss: 0.1553, Validation loss 0.1665\n",
      "Epoch [13/200], Train loss: 0.1516, Validation loss 0.1471\n",
      "Epoch [14/200], Train loss: 0.1753, Validation loss 0.1754\n",
      "Epoch [15/200], Train loss: 0.1690, Validation loss 0.1485\n",
      "Epoch [16/200], Train loss: 0.1832, Validation loss 0.1491\n",
      "Epoch [17/200], Train loss: 0.1183, Validation loss 0.1216\n",
      "Epoch [18/200], Train loss: 0.1196, Validation loss 0.1187\n",
      "Epoch [19/200], Train loss: 0.0992, Validation loss 0.1191\n",
      "Epoch [20/200], Train loss: 0.0870, Validation loss 0.1088\n",
      "Epoch [21/200], Train loss: 0.0525, Validation loss 0.1338\n",
      "Epoch [22/200], Train loss: 0.1102, Validation loss 0.1684\n",
      "Epoch [23/200], Train loss: 0.1291, Validation loss 0.1025\n",
      "Epoch [24/200], Train loss: 0.1073, Validation loss 0.1026\n",
      "Epoch [25/200], Train loss: 0.1193, Validation loss 0.1065\n",
      "Epoch [26/200], Train loss: 0.1111, Validation loss 0.1018\n",
      "Epoch [27/200], Train loss: 0.0374, Validation loss 0.0982\n",
      "Epoch [28/200], Train loss: 0.0915, Validation loss 0.0977\n",
      "Epoch [29/200], Train loss: 0.1088, Validation loss 0.0929\n",
      "Epoch [30/200], Train loss: 0.1076, Validation loss 0.0905\n",
      "Epoch [31/200], Train loss: 0.0894, Validation loss 0.0941\n",
      "Epoch [32/200], Train loss: 0.2908, Validation loss 0.2190\n",
      "Epoch [33/200], Train loss: 0.1751, Validation loss 0.1600\n",
      "Epoch [34/200], Train loss: 0.1116, Validation loss 0.0959\n",
      "Epoch [35/200], Train loss: 0.1383, Validation loss 0.1194\n",
      "Epoch [36/200], Train loss: 0.1010, Validation loss 0.1120\n",
      "Epoch [37/200], Train loss: 0.0886, Validation loss 0.0900\n",
      "Epoch [38/200], Train loss: 0.0676, Validation loss 0.0869\n",
      "Epoch [39/200], Train loss: 0.0877, Validation loss 0.0942\n",
      "Epoch [40/200], Train loss: 0.1096, Validation loss 0.0844\n",
      "Epoch [41/200], Train loss: 0.0847, Validation loss 0.0843\n",
      "Epoch [42/200], Train loss: 0.0970, Validation loss 0.0807\n",
      "Epoch [43/200], Train loss: 0.1179, Validation loss 0.1007\n",
      "Epoch [44/200], Train loss: 0.1266, Validation loss 0.0829\n",
      "Epoch [45/200], Train loss: 0.1189, Validation loss 0.1121\n",
      "Epoch [46/200], Train loss: 0.1027, Validation loss 0.0837\n",
      "Epoch [47/200], Train loss: 0.0998, Validation loss 0.0900\n",
      "Epoch [48/200], Train loss: 0.0555, Validation loss 0.0866\n",
      "Epoch [49/200], Train loss: 0.0658, Validation loss 0.0815\n",
      "Epoch [50/200], Train loss: 0.1041, Validation loss 0.0892\n",
      "Epoch [51/200], Train loss: 0.1128, Validation loss 0.0861\n",
      "Epoch [52/200], Train loss: 0.1006, Validation loss 0.0764\n",
      "Epoch [53/200], Train loss: 0.1504, Validation loss 0.0885\n",
      "Epoch [54/200], Train loss: 0.0935, Validation loss 0.0822\n",
      "Epoch [55/200], Train loss: 0.2304, Validation loss 0.2033\n",
      "Epoch [56/200], Train loss: 0.1299, Validation loss 0.1229\n",
      "Epoch [57/200], Train loss: 0.0690, Validation loss 0.1193\n",
      "Epoch [58/200], Train loss: 0.0889, Validation loss 0.0970\n",
      "Epoch [59/200], Train loss: 0.0720, Validation loss 0.0796\n",
      "Epoch [60/200], Train loss: 0.0834, Validation loss 0.0827\n",
      "Epoch [61/200], Train loss: 0.0914, Validation loss 0.0773\n",
      "Epoch [62/200], Train loss: 0.0699, Validation loss 0.0780\n",
      "Epoch [63/200], Train loss: 0.0609, Validation loss 0.0918\n",
      "Epoch [64/200], Train loss: 0.0877, Validation loss 0.0885\n",
      "Epoch [65/200], Train loss: 0.1105, Validation loss 0.0786\n",
      "Epoch [66/200], Train loss: 0.0810, Validation loss 0.0823\n",
      "Epoch [67/200], Train loss: 0.0862, Validation loss 0.0830\n",
      "Epoch [68/200], Train loss: 0.0695, Validation loss 0.0828\n",
      "Epoch [69/200], Train loss: 0.0899, Validation loss 0.0814\n",
      "Epoch [70/200], Train loss: 0.0678, Validation loss 0.0894\n",
      "Epoch [71/200], Train loss: 0.0676, Validation loss 0.0868\n",
      "Training finished! after 71 epochs, Model saved to ./model-fold-0.pth\n",
      "Epoch [1/200], Train loss: 0.4932, Validation loss 0.5340\n",
      "Epoch [2/200], Train loss: 0.7441, Validation loss 0.6096\n",
      "Epoch [3/200], Train loss: 0.2729, Validation loss 0.4149\n",
      "Epoch [4/200], Train loss: 0.6112, Validation loss 0.3614\n",
      "Epoch [5/200], Train loss: 0.4043, Validation loss 0.5635\n",
      "Epoch [6/200], Train loss: 0.5833, Validation loss 0.5219\n",
      "Epoch [7/200], Train loss: 0.4563, Validation loss 0.3783\n",
      "Epoch [8/200], Train loss: 0.6921, Validation loss 0.5110\n",
      "Epoch [9/200], Train loss: 0.2321, Validation loss 0.1785\n",
      "Epoch [10/200], Train loss: 0.1398, Validation loss 0.1837\n",
      "Epoch [11/200], Train loss: 0.0857, Validation loss 0.2224\n",
      "Epoch [12/200], Train loss: 0.7612, Validation loss 0.6425\n",
      "Epoch [13/200], Train loss: 0.4387, Validation loss 0.2360\n",
      "Epoch [14/200], Train loss: 0.1316, Validation loss 0.2206\n",
      "Epoch [15/200], Train loss: 0.0915, Validation loss 0.2514\n",
      "Epoch [16/200], Train loss: 0.1198, Validation loss 0.2214\n",
      "Epoch [17/200], Train loss: 0.0824, Validation loss 0.2007\n",
      "Epoch [18/200], Train loss: 0.3111, Validation loss 0.2153\n",
      "Epoch [19/200], Train loss: 0.1012, Validation loss 0.2156\n",
      "Epoch [20/200], Train loss: 0.1193, Validation loss 0.2187\n",
      "Epoch [21/200], Train loss: 0.0996, Validation loss 0.1804\n",
      "Epoch [22/200], Train loss: 0.1095, Validation loss 0.2213\n",
      "Epoch [23/200], Train loss: 0.3044, Validation loss 0.1894\n",
      "Epoch [24/200], Train loss: 0.0691, Validation loss 0.2500\n",
      "Epoch [25/200], Train loss: 0.0588, Validation loss 0.1630\n",
      "Epoch [26/200], Train loss: 0.0689, Validation loss 0.1755\n",
      "Epoch [27/200], Train loss: 0.1607, Validation loss 0.2079\n",
      "Epoch [28/200], Train loss: 0.1373, Validation loss 0.2006\n",
      "Epoch [29/200], Train loss: 0.4081, Validation loss 0.2132\n",
      "Epoch [30/200], Train loss: 0.0981, Validation loss 0.2534\n",
      "Epoch [31/200], Train loss: 0.0864, Validation loss 0.2242\n",
      "Epoch [32/200], Train loss: 0.0853, Validation loss 0.2361\n",
      "Epoch [33/200], Train loss: 0.0620, Validation loss 0.2084\n",
      "Epoch [34/200], Train loss: 0.0604, Validation loss 0.2609\n",
      "Epoch [35/200], Train loss: 0.0317, Validation loss 0.2302\n",
      "Epoch [36/200], Train loss: 0.1278, Validation loss 0.2210\n",
      "Epoch [37/200], Train loss: 0.0145, Validation loss 0.2280\n",
      "Epoch [38/200], Train loss: 0.0843, Validation loss 0.1639\n",
      "Epoch [39/200], Train loss: 0.2342, Validation loss 0.1671\n",
      "Epoch [40/200], Train loss: 0.7683, Validation loss 0.2892\n",
      "Epoch [41/200], Train loss: 0.0425, Validation loss 0.2350\n",
      "Epoch [42/200], Train loss: 0.2062, Validation loss 0.2300\n",
      "Epoch [43/200], Train loss: 0.7934, Validation loss 0.2211\n",
      "Epoch [44/200], Train loss: 0.0776, Validation loss 0.2279\n",
      "Epoch [45/200], Train loss: 0.0592, Validation loss 0.2185\n",
      "Epoch [46/200], Train loss: 0.0774, Validation loss 0.1861\n",
      "Epoch [47/200], Train loss: 0.6847, Validation loss 0.2319\n",
      "Epoch [48/200], Train loss: 0.0870, Validation loss 0.2234\n",
      "Epoch [49/200], Train loss: 1.1103, Validation loss 0.2367\n",
      "Epoch [50/200], Train loss: 0.0569, Validation loss 0.2385\n",
      "Epoch [51/200], Train loss: 0.6326, Validation loss 0.2597\n",
      "Epoch [52/200], Train loss: 0.6060, Validation loss 0.2507\n",
      "Epoch [53/200], Train loss: 0.0124, Validation loss 0.2650\n",
      "Epoch [54/200], Train loss: 0.8496, Validation loss 0.1931\n",
      "Epoch [55/200], Train loss: 0.2986, Validation loss 0.2244\n",
      "Epoch [56/200], Train loss: 0.8486, Validation loss 0.2100\n",
      "Epoch [57/200], Train loss: 0.0397, Validation loss 0.2216\n",
      "Epoch [58/200], Train loss: 0.5771, Validation loss 0.2685\n",
      "Epoch [59/200], Train loss: 0.3332, Validation loss 0.2710\n",
      "Epoch [60/200], Train loss: 0.0324, Validation loss 0.2455\n",
      "Epoch [61/200], Train loss: 0.5711, Validation loss 0.2923\n",
      "Epoch [62/200], Train loss: 0.1300, Validation loss 0.2022\n",
      "Epoch [63/200], Train loss: 0.1514, Validation loss 0.2999\n",
      "Epoch [64/200], Train loss: 0.3217, Validation loss 0.2105\n",
      "Epoch [65/200], Train loss: 0.0640, Validation loss 0.2525\n",
      "Epoch [66/200], Train loss: 0.0348, Validation loss 0.2454\n",
      "Epoch [67/200], Train loss: 0.1014, Validation loss 0.2376\n",
      "Epoch [68/200], Train loss: 0.0476, Validation loss 0.2026\n",
      "Epoch [69/200], Train loss: 0.0866, Validation loss 0.2552\n",
      "Epoch [70/200], Train loss: 0.3769, Validation loss 0.2891\n",
      "Epoch [71/200], Train loss: 0.3113, Validation loss 0.2229\n",
      "Epoch [72/200], Train loss: 0.5202, Validation loss 0.2602\n",
      "Epoch [73/200], Train loss: 0.0287, Validation loss 0.2385\n",
      "Training finished! after 73 epochs, Model saved to ./model-fold-1.pth\n",
      "Epoch [1/200], Train loss: 0.6540, Validation loss 0.6171\n",
      "Epoch [2/200], Train loss: 0.9061, Validation loss 0.6201\n",
      "Epoch [3/200], Train loss: 0.3738, Validation loss 0.4369\n",
      "Epoch [4/200], Train loss: 0.3672, Validation loss 0.3611\n",
      "Epoch [5/200], Train loss: 0.6911, Validation loss 0.5592\n",
      "Epoch [6/200], Train loss: 0.3200, Validation loss 0.3080\n",
      "Epoch [7/200], Train loss: 0.2643, Validation loss 0.2897\n",
      "Epoch [8/200], Train loss: 0.2597, Validation loss 0.2179\n",
      "Epoch [9/200], Train loss: 0.2120, Validation loss 0.2136\n",
      "Epoch [10/200], Train loss: 0.2471, Validation loss 0.2246\n",
      "Epoch [11/200], Train loss: 0.2614, Validation loss 0.2472\n",
      "Epoch [12/200], Train loss: 0.0983, Validation loss 0.1924\n",
      "Epoch [13/200], Train loss: 0.1715, Validation loss 0.2073\n",
      "Epoch [14/200], Train loss: 0.1285, Validation loss 0.2007\n",
      "Epoch [15/200], Train loss: 0.1547, Validation loss 0.1801\n",
      "Epoch [16/200], Train loss: 0.1452, Validation loss 0.1633\n",
      "Epoch [17/200], Train loss: 0.2044, Validation loss 0.1848\n",
      "Epoch [18/200], Train loss: 0.3520, Validation loss 0.3065\n",
      "Epoch [19/200], Train loss: 0.1280, Validation loss 0.1612\n",
      "Epoch [20/200], Train loss: 0.1988, Validation loss 0.1622\n",
      "Epoch [21/200], Train loss: 0.1620, Validation loss 0.2131\n",
      "Epoch [22/200], Train loss: 0.1397, Validation loss 0.1682\n",
      "Epoch [23/200], Train loss: 0.1492, Validation loss 0.1652\n",
      "Epoch [24/200], Train loss: 0.4444, Validation loss 0.1987\n",
      "Epoch [25/200], Train loss: 0.1238, Validation loss 0.1345\n",
      "Epoch [26/200], Train loss: 0.1588, Validation loss 0.1274\n",
      "Epoch [27/200], Train loss: 0.1204, Validation loss 0.1409\n",
      "Epoch [28/200], Train loss: 0.1354, Validation loss 0.1412\n",
      "Epoch [29/200], Train loss: 0.1497, Validation loss 0.1449\n",
      "Epoch [30/200], Train loss: 0.1084, Validation loss 0.1401\n",
      "Epoch [31/200], Train loss: 0.1089, Validation loss 0.1257\n",
      "Epoch [32/200], Train loss: 0.0775, Validation loss 0.1335\n",
      "Epoch [33/200], Train loss: 0.1026, Validation loss 0.1283\n",
      "Epoch [34/200], Train loss: 0.1403, Validation loss 0.1209\n",
      "Epoch [35/200], Train loss: 0.0970, Validation loss 0.1415\n",
      "Epoch [36/200], Train loss: 0.1201, Validation loss 0.2026\n",
      "Epoch [37/200], Train loss: 0.1080, Validation loss 0.1313\n",
      "Epoch [38/200], Train loss: 0.1962, Validation loss 0.1327\n",
      "Epoch [39/200], Train loss: 0.1114, Validation loss 0.1244\n",
      "Epoch [40/200], Train loss: 0.0832, Validation loss 0.1337\n",
      "Epoch [41/200], Train loss: 0.1569, Validation loss 0.1476\n",
      "Epoch [42/200], Train loss: 0.2643, Validation loss 0.1297\n",
      "Epoch [43/200], Train loss: 0.4277, Validation loss 0.1204\n",
      "Epoch [44/200], Train loss: 0.0774, Validation loss 0.1296\n",
      "Epoch [45/200], Train loss: 0.0999, Validation loss 0.1367\n",
      "Epoch [46/200], Train loss: 0.0602, Validation loss 0.1553\n",
      "Epoch [47/200], Train loss: 0.1935, Validation loss 0.1243\n",
      "Epoch [48/200], Train loss: 0.0979, Validation loss 0.1348\n",
      "Epoch [49/200], Train loss: 0.1057, Validation loss 0.0993\n",
      "Epoch [50/200], Train loss: 0.0501, Validation loss 0.1152\n",
      "Epoch [51/200], Train loss: 0.0813, Validation loss 0.1156\n",
      "Epoch [52/200], Train loss: 0.9550, Validation loss 1.0608\n",
      "Epoch [53/200], Train loss: 0.1586, Validation loss 0.2148\n",
      "Epoch [54/200], Train loss: 0.1740, Validation loss 0.1260\n",
      "Epoch [55/200], Train loss: 0.1058, Validation loss 0.1338\n",
      "Epoch [56/200], Train loss: 0.0846, Validation loss 0.1285\n",
      "Epoch [57/200], Train loss: 0.1553, Validation loss 0.1575\n",
      "Epoch [58/200], Train loss: 0.4006, Validation loss 0.1244\n",
      "Epoch [59/200], Train loss: 0.1845, Validation loss 0.1272\n",
      "Epoch [60/200], Train loss: 0.1025, Validation loss 0.1221\n",
      "Epoch [61/200], Train loss: 0.1298, Validation loss 0.1376\n",
      "Epoch [62/200], Train loss: 0.1019, Validation loss 0.1103\n",
      "Epoch [63/200], Train loss: 0.0978, Validation loss 0.1472\n",
      "Epoch [64/200], Train loss: 0.0769, Validation loss 0.1515\n",
      "Epoch [65/200], Train loss: 0.1229, Validation loss 0.1236\n",
      "Epoch [66/200], Train loss: 0.3160, Validation loss 0.1302\n",
      "Epoch [67/200], Train loss: 0.0436, Validation loss 0.1223\n",
      "Epoch [68/200], Train loss: 0.0687, Validation loss 0.1167\n",
      "Epoch [69/200], Train loss: 0.1175, Validation loss 0.1168\n",
      "Epoch [70/200], Train loss: 0.0772, Validation loss 0.1412\n",
      "Epoch [71/200], Train loss: 0.1103, Validation loss 0.1187\n",
      "Epoch [72/200], Train loss: 0.0778, Validation loss 0.1481\n",
      "Epoch [73/200], Train loss: 0.1020, Validation loss 0.1325\n",
      "Epoch [74/200], Train loss: 0.0800, Validation loss 0.1312\n",
      "Epoch [75/200], Train loss: 0.2371, Validation loss 0.8589\n",
      "Epoch [76/200], Train loss: 0.3682, Validation loss 0.2444\n",
      "Epoch [77/200], Train loss: 0.2808, Validation loss 0.1272\n",
      "Epoch [78/200], Train loss: 0.2271, Validation loss 0.1269\n",
      "Epoch [79/200], Train loss: 0.0868, Validation loss 0.1306\n",
      "Epoch [80/200], Train loss: 0.0992, Validation loss 0.1151\n",
      "Epoch [81/200], Train loss: 0.1071, Validation loss 0.1203\n",
      "Training finished! after 81 epochs, Model saved to ./model-fold-2.pth\n",
      "Epoch [1/200], Train loss: 0.6340, Validation loss 0.6666\n",
      "Epoch [2/200], Train loss: 0.6150, Validation loss 0.7519\n",
      "Epoch [3/200], Train loss: 0.4531, Validation loss 0.5087\n",
      "Epoch [4/200], Train loss: 0.3545, Validation loss 0.4226\n",
      "Epoch [5/200], Train loss: 0.4131, Validation loss 0.3867\n",
      "Epoch [6/200], Train loss: 0.2643, Validation loss 0.3416\n",
      "Epoch [7/200], Train loss: 0.3000, Validation loss 0.4018\n",
      "Epoch [8/200], Train loss: 0.4386, Validation loss 0.3736\n",
      "Epoch [9/200], Train loss: 0.2197, Validation loss 0.3758\n",
      "Epoch [10/200], Train loss: 0.2279, Validation loss 0.2753\n",
      "Epoch [11/200], Train loss: 0.1831, Validation loss 0.2268\n",
      "Epoch [12/200], Train loss: 0.4103, Validation loss 0.2314\n",
      "Epoch [13/200], Train loss: 0.1887, Validation loss 0.2476\n",
      "Epoch [14/200], Train loss: 0.3453, Validation loss 0.2690\n",
      "Epoch [15/200], Train loss: 0.4350, Validation loss 0.2068\n",
      "Epoch [16/200], Train loss: 0.4981, Validation loss 0.2236\n",
      "Epoch [17/200], Train loss: 0.1435, Validation loss 0.2278\n",
      "Epoch [18/200], Train loss: 0.3441, Validation loss 0.3180\n",
      "Epoch [19/200], Train loss: 0.4176, Validation loss 0.2487\n",
      "Epoch [20/200], Train loss: 0.1243, Validation loss 0.2133\n",
      "Epoch [21/200], Train loss: 0.1204, Validation loss 0.2471\n",
      "Epoch [22/200], Train loss: 0.0967, Validation loss 0.2044\n",
      "Epoch [23/200], Train loss: 0.6447, Validation loss 0.2313\n",
      "Epoch [24/200], Train loss: 0.1054, Validation loss 0.2102\n",
      "Epoch [25/200], Train loss: 0.0764, Validation loss 0.1997\n",
      "Epoch [26/200], Train loss: 0.3267, Validation loss 0.2236\n",
      "Epoch [27/200], Train loss: 0.1037, Validation loss 0.2007\n",
      "Epoch [28/200], Train loss: 0.1026, Validation loss 0.2340\n",
      "Epoch [29/200], Train loss: 0.1072, Validation loss 0.1412\n",
      "Epoch [30/200], Train loss: 0.1535, Validation loss 0.1771\n",
      "Epoch [31/200], Train loss: 0.1702, Validation loss 0.2402\n",
      "Epoch [32/200], Train loss: 0.2327, Validation loss 0.2178\n",
      "Epoch [33/200], Train loss: 0.1436, Validation loss 0.2307\n",
      "Epoch [34/200], Train loss: 0.0743, Validation loss 0.1739\n",
      "Epoch [35/200], Train loss: 0.3313, Validation loss 0.2335\n",
      "Epoch [36/200], Train loss: 0.2630, Validation loss 0.1826\n",
      "Epoch [37/200], Train loss: 0.8315, Validation loss 1.9889\n",
      "Epoch [38/200], Train loss: 0.0960, Validation loss 0.1676\n",
      "Epoch [39/200], Train loss: 0.3014, Validation loss 0.1923\n",
      "Epoch [40/200], Train loss: 0.0863, Validation loss 0.2481\n",
      "Epoch [41/200], Train loss: 0.0993, Validation loss 0.2181\n",
      "Epoch [42/200], Train loss: 0.1241, Validation loss 0.2664\n",
      "Epoch [43/200], Train loss: 0.0805, Validation loss 0.2572\n",
      "Epoch [44/200], Train loss: 0.8409, Validation loss 0.2693\n",
      "Epoch [45/200], Train loss: 0.0721, Validation loss 0.2327\n",
      "Epoch [46/200], Train loss: 0.1296, Validation loss 0.1970\n",
      "Epoch [47/200], Train loss: 0.0892, Validation loss 0.1370\n",
      "Epoch [48/200], Train loss: 0.0884, Validation loss 0.1789\n",
      "Epoch [49/200], Train loss: 0.0954, Validation loss 0.1791\n",
      "Epoch [50/200], Train loss: 0.4379, Validation loss 0.1863\n",
      "Epoch [51/200], Train loss: 0.0626, Validation loss 0.2215\n",
      "Epoch [52/200], Train loss: 0.1024, Validation loss 0.2382\n",
      "Epoch [53/200], Train loss: 0.0372, Validation loss 0.2211\n",
      "Epoch [54/200], Train loss: 0.6831, Validation loss 0.2166\n",
      "Epoch [55/200], Train loss: 0.1000, Validation loss 0.2393\n",
      "Epoch [56/200], Train loss: 0.0949, Validation loss 0.2181\n",
      "Epoch [57/200], Train loss: 0.0799, Validation loss 0.3159\n",
      "Epoch [58/200], Train loss: 1.0007, Validation loss 0.2818\n",
      "Epoch [59/200], Train loss: 0.0791, Validation loss 0.2015\n",
      "Epoch [60/200], Train loss: 0.1221, Validation loss 0.2421\n",
      "Epoch [61/200], Train loss: 0.1121, Validation loss 0.3091\n",
      "Epoch [62/200], Train loss: 0.5996, Validation loss 0.2196\n",
      "Epoch [63/200], Train loss: 0.2972, Validation loss 0.2033\n",
      "Epoch [64/200], Train loss: 0.0423, Validation loss 0.2533\n",
      "Epoch [65/200], Train loss: 0.0955, Validation loss 0.1325\n",
      "Epoch [66/200], Train loss: 0.6288, Validation loss 0.2761\n",
      "Epoch [67/200], Train loss: 0.1001, Validation loss 0.2478\n",
      "Epoch [68/200], Train loss: 0.0697, Validation loss 0.2280\n",
      "Epoch [69/200], Train loss: 0.0624, Validation loss 0.2321\n",
      "Epoch [70/200], Train loss: 0.6815, Validation loss 0.2272\n",
      "Epoch [71/200], Train loss: 0.1369, Validation loss 0.1531\n",
      "Epoch [72/200], Train loss: 0.3997, Validation loss 0.2092\n",
      "Epoch [73/200], Train loss: 0.0555, Validation loss 0.2256\n",
      "Epoch [74/200], Train loss: 0.1414, Validation loss 0.2976\n",
      "Epoch [75/200], Train loss: 0.0747, Validation loss 0.2440\n",
      "Epoch [76/200], Train loss: 0.0276, Validation loss 0.3192\n",
      "Epoch [77/200], Train loss: 0.1693, Validation loss 0.2614\n",
      "Epoch [78/200], Train loss: 0.0979, Validation loss 0.2737\n",
      "Epoch [79/200], Train loss: 0.0517, Validation loss 0.3255\n",
      "Epoch [80/200], Train loss: 0.3314, Validation loss 0.3096\n",
      "Epoch [81/200], Train loss: 0.0794, Validation loss 0.2380\n",
      "Epoch [82/200], Train loss: 0.0931, Validation loss 0.2627\n",
      "Epoch [83/200], Train loss: 1.0972, Validation loss 0.3197\n",
      "Epoch [84/200], Train loss: 0.6022, Validation loss 0.3104\n",
      "Training finished! after 84 epochs, Model saved to ./model-fold-3.pth\n",
      "Epoch [1/200], Train loss: 0.7118, Validation loss 0.7220\n",
      "Epoch [2/200], Train loss: 0.3597, Validation loss 0.4094\n",
      "Epoch [3/200], Train loss: 0.4278, Validation loss 0.4454\n",
      "Epoch [4/200], Train loss: 0.3789, Validation loss 0.3309\n",
      "Epoch [5/200], Train loss: 0.2783, Validation loss 0.2771\n",
      "Epoch [6/200], Train loss: 0.3706, Validation loss 0.4178\n",
      "Epoch [7/200], Train loss: 0.2297, Validation loss 0.2416\n",
      "Epoch [8/200], Train loss: 0.2236, Validation loss 0.2145\n",
      "Epoch [9/200], Train loss: 0.1665, Validation loss 0.1922\n",
      "Epoch [10/200], Train loss: 0.1625, Validation loss 0.1880\n",
      "Epoch [11/200], Train loss: 0.1349, Validation loss 0.1614\n",
      "Epoch [12/200], Train loss: 0.1662, Validation loss 0.1583\n",
      "Epoch [13/200], Train loss: 0.1524, Validation loss 0.1470\n",
      "Epoch [14/200], Train loss: 0.1370, Validation loss 0.1365\n",
      "Epoch [15/200], Train loss: 0.1400, Validation loss 0.1335\n",
      "Epoch [16/200], Train loss: 0.1216, Validation loss 0.1671\n",
      "Epoch [17/200], Train loss: 0.1028, Validation loss 0.1313\n",
      "Epoch [18/200], Train loss: 0.1414, Validation loss 0.1157\n",
      "Epoch [19/200], Train loss: 0.2234, Validation loss 0.1293\n",
      "Epoch [20/200], Train loss: 0.1423, Validation loss 0.1878\n",
      "Epoch [21/200], Train loss: 0.0966, Validation loss 0.1114\n",
      "Epoch [22/200], Train loss: 0.1246, Validation loss 0.1083\n",
      "Epoch [23/200], Train loss: 0.0512, Validation loss 0.1076\n",
      "Epoch [24/200], Train loss: 0.0980, Validation loss 0.1023\n",
      "Epoch [25/200], Train loss: 0.1525, Validation loss 0.0961\n",
      "Epoch [26/200], Train loss: 0.0750, Validation loss 0.1004\n",
      "Epoch [27/200], Train loss: 0.0882, Validation loss 0.0990\n",
      "Epoch [28/200], Train loss: 0.1175, Validation loss 0.0931\n",
      "Epoch [29/200], Train loss: 0.0959, Validation loss 0.0921\n",
      "Epoch [30/200], Train loss: 0.0956, Validation loss 0.0963\n",
      "Epoch [31/200], Train loss: 0.0630, Validation loss 0.0971\n",
      "Epoch [32/200], Train loss: 0.0858, Validation loss 0.1002\n",
      "Epoch [33/200], Train loss: 0.1375, Validation loss 0.1283\n",
      "Epoch [34/200], Train loss: 0.1324, Validation loss 0.0978\n",
      "Epoch [35/200], Train loss: 0.1419, Validation loss 0.1099\n",
      "Epoch [36/200], Train loss: 0.0464, Validation loss 0.0881\n",
      "Epoch [37/200], Train loss: 0.0691, Validation loss 0.0870\n",
      "Epoch [38/200], Train loss: 0.0493, Validation loss 0.0908\n",
      "Epoch [39/200], Train loss: 0.0762, Validation loss 0.0817\n",
      "Epoch [40/200], Train loss: 0.2212, Validation loss 0.1465\n",
      "Epoch [41/200], Train loss: 0.1156, Validation loss 0.0875\n",
      "Epoch [42/200], Train loss: 0.1265, Validation loss 0.0913\n",
      "Epoch [43/200], Train loss: 0.1012, Validation loss 0.0878\n",
      "Epoch [44/200], Train loss: 0.0847, Validation loss 0.0813\n",
      "Epoch [45/200], Train loss: 0.0427, Validation loss 0.0797\n",
      "Epoch [46/200], Train loss: 0.2783, Validation loss 0.1368\n",
      "Epoch [47/200], Train loss: 0.0840, Validation loss 0.0817\n",
      "Epoch [48/200], Train loss: 0.0861, Validation loss 0.0833\n",
      "Epoch [49/200], Train loss: 0.0446, Validation loss 0.0782\n",
      "Epoch [50/200], Train loss: 0.0765, Validation loss 0.0786\n",
      "Epoch [51/200], Train loss: 0.0841, Validation loss 0.0808\n",
      "Epoch [52/200], Train loss: 0.1271, Validation loss 0.0815\n",
      "Epoch [53/200], Train loss: 0.0499, Validation loss 0.0856\n",
      "Epoch [54/200], Train loss: 0.0946, Validation loss 0.0932\n",
      "Epoch [55/200], Train loss: 0.0817, Validation loss 0.1079\n",
      "Epoch [56/200], Train loss: 0.1073, Validation loss 0.0756\n",
      "Epoch [57/200], Train loss: 0.0969, Validation loss 0.0909\n",
      "Epoch [58/200], Train loss: 0.1026, Validation loss 0.0760\n",
      "Epoch [59/200], Train loss: 0.0885, Validation loss 0.0733\n",
      "Epoch [60/200], Train loss: 0.0215, Validation loss 0.0843\n",
      "Epoch [61/200], Train loss: 0.0967, Validation loss 0.0980\n",
      "Epoch [62/200], Train loss: 0.2234, Validation loss 0.3285\n",
      "Epoch [63/200], Train loss: 0.0665, Validation loss 0.0881\n",
      "Epoch [64/200], Train loss: 0.0400, Validation loss 0.0734\n",
      "Epoch [65/200], Train loss: 0.0311, Validation loss 0.0981\n",
      "Epoch [66/200], Train loss: 0.0698, Validation loss 0.0927\n",
      "Epoch [67/200], Train loss: 0.0747, Validation loss 0.0789\n",
      "Epoch [68/200], Train loss: 0.0850, Validation loss 0.0767\n",
      "Epoch [69/200], Train loss: 0.0828, Validation loss 0.0797\n",
      "Epoch [70/200], Train loss: 0.0715, Validation loss 0.0774\n",
      "Epoch [71/200], Train loss: 0.0349, Validation loss 0.0785\n",
      "Epoch [72/200], Train loss: 0.1196, Validation loss 0.0783\n",
      "Epoch [73/200], Train loss: 0.0831, Validation loss 0.0832\n",
      "Epoch [74/200], Train loss: 0.0688, Validation loss 0.0786\n",
      "Epoch [75/200], Train loss: 0.0566, Validation loss 0.0747\n",
      "Epoch [76/200], Train loss: 2.0990, Validation loss 1.3155\n",
      "Epoch [77/200], Train loss: 0.0688, Validation loss 0.0908\n",
      "Epoch [78/200], Train loss: 0.0690, Validation loss 0.1333\n",
      "Training finished! after 78 epochs, Model saved to ./model-fold-4.pth\n",
      "Epoch [1/200], Train loss: 0.7721, Validation loss 0.7883\n",
      "Epoch [2/200], Train loss: 0.6351, Validation loss 0.5913\n",
      "Epoch [3/200], Train loss: 0.5749, Validation loss 0.5711\n",
      "Epoch [4/200], Train loss: 0.3609, Validation loss 0.4038\n",
      "Epoch [5/200], Train loss: 0.3829, Validation loss 0.3725\n",
      "Epoch [6/200], Train loss: 0.4788, Validation loss 0.3709\n",
      "Epoch [7/200], Train loss: 0.2622, Validation loss 0.3108\n",
      "Epoch [8/200], Train loss: 0.2498, Validation loss 0.2926\n",
      "Epoch [9/200], Train loss: 0.3178, Validation loss 0.2924\n",
      "Epoch [10/200], Train loss: 0.1952, Validation loss 0.2184\n",
      "Epoch [11/200], Train loss: 0.1658, Validation loss 0.1950\n",
      "Epoch [12/200], Train loss: 0.1898, Validation loss 0.2047\n",
      "Epoch [13/200], Train loss: 0.1758, Validation loss 0.2153\n",
      "Epoch [14/200], Train loss: 0.1683, Validation loss 0.1647\n",
      "Epoch [15/200], Train loss: 0.1107, Validation loss 0.1525\n",
      "Epoch [16/200], Train loss: 0.4961, Validation loss 0.4659\n",
      "Epoch [17/200], Train loss: 0.2263, Validation loss 0.2014\n",
      "Epoch [18/200], Train loss: 0.1530, Validation loss 0.1535\n",
      "Epoch [19/200], Train loss: 0.1827, Validation loss 0.1425\n",
      "Epoch [20/200], Train loss: 0.2293, Validation loss 0.1449\n",
      "Epoch [21/200], Train loss: 0.1167, Validation loss 0.1554\n",
      "Epoch [22/200], Train loss: 0.1788, Validation loss 0.1466\n",
      "Epoch [23/200], Train loss: 0.1291, Validation loss 0.1240\n",
      "Epoch [24/200], Train loss: 0.1585, Validation loss 0.3143\n",
      "Epoch [25/200], Train loss: 0.1942, Validation loss 0.1605\n",
      "Epoch [26/200], Train loss: 0.1524, Validation loss 0.1189\n",
      "Epoch [27/200], Train loss: 0.3235, Validation loss 0.3772\n",
      "Epoch [28/200], Train loss: 0.0787, Validation loss 0.1208\n",
      "Epoch [29/200], Train loss: 0.1019, Validation loss 0.1333\n",
      "Epoch [30/200], Train loss: 0.1249, Validation loss 0.1050\n",
      "Epoch [31/200], Train loss: 0.0916, Validation loss 0.1078\n",
      "Epoch [32/200], Train loss: 0.0970, Validation loss 0.2241\n",
      "Epoch [33/200], Train loss: 0.1077, Validation loss 0.1224\n",
      "Epoch [34/200], Train loss: 0.0768, Validation loss 0.1038\n",
      "Epoch [35/200], Train loss: 0.0866, Validation loss 0.1069\n",
      "Epoch [36/200], Train loss: 0.0757, Validation loss 0.0982\n",
      "Epoch [37/200], Train loss: 0.1174, Validation loss 0.1003\n",
      "Epoch [38/200], Train loss: 0.1298, Validation loss 0.0985\n",
      "Epoch [39/200], Train loss: 0.0835, Validation loss 0.0963\n",
      "Epoch [40/200], Train loss: 0.0880, Validation loss 0.1096\n",
      "Epoch [41/200], Train loss: 0.1017, Validation loss 0.1047\n",
      "Epoch [42/200], Train loss: 0.0489, Validation loss 0.0980\n",
      "Epoch [43/200], Train loss: 0.0595, Validation loss 0.1180\n",
      "Epoch [44/200], Train loss: 0.5046, Validation loss 0.4656\n",
      "Epoch [45/200], Train loss: 0.0929, Validation loss 0.1152\n",
      "Epoch [46/200], Train loss: 0.1049, Validation loss 0.1098\n",
      "Epoch [47/200], Train loss: 0.0788, Validation loss 0.0988\n",
      "Epoch [48/200], Train loss: 0.1047, Validation loss 0.0998\n",
      "Epoch [49/200], Train loss: 0.0632, Validation loss 0.0932\n",
      "Epoch [50/200], Train loss: 0.1069, Validation loss 0.0960\n",
      "Epoch [51/200], Train loss: 0.0488, Validation loss 0.1021\n",
      "Epoch [52/200], Train loss: 0.1276, Validation loss 0.0887\n",
      "Epoch [53/200], Train loss: 0.0880, Validation loss 0.0862\n",
      "Epoch [54/200], Train loss: 0.1035, Validation loss 0.0885\n",
      "Epoch [55/200], Train loss: 0.0532, Validation loss 0.0976\n",
      "Epoch [56/200], Train loss: 0.0589, Validation loss 0.0915\n",
      "Epoch [57/200], Train loss: 0.0999, Validation loss 0.1034\n",
      "Epoch [58/200], Train loss: 0.0585, Validation loss 0.0890\n",
      "Epoch [59/200], Train loss: 0.0967, Validation loss 0.0937\n",
      "Epoch [60/200], Train loss: 0.1176, Validation loss 0.0884\n",
      "Epoch [61/200], Train loss: 0.0604, Validation loss 0.0943\n",
      "Epoch [62/200], Train loss: 0.0978, Validation loss 0.0943\n",
      "Epoch [63/200], Train loss: 0.0878, Validation loss 0.1216\n",
      "Epoch [64/200], Train loss: 0.0783, Validation loss 0.1058\n",
      "Epoch [65/200], Train loss: 0.1773, Validation loss 0.1334\n",
      "Epoch [66/200], Train loss: 0.1551, Validation loss 0.0993\n",
      "Epoch [67/200], Train loss: 0.0869, Validation loss 0.1078\n",
      "Epoch [68/200], Train loss: 0.0895, Validation loss 0.0880\n",
      "Epoch [69/200], Train loss: 0.0745, Validation loss 0.0843\n",
      "Epoch [70/200], Train loss: 0.0972, Validation loss 0.1982\n",
      "Epoch [71/200], Train loss: 0.0821, Validation loss 0.0936\n",
      "Epoch [72/200], Train loss: 0.1571, Validation loss 0.1498\n",
      "Epoch [73/200], Train loss: 0.5676, Validation loss 0.0900\n",
      "Epoch [74/200], Train loss: 0.1228, Validation loss 0.1082\n",
      "Epoch [75/200], Train loss: 0.1258, Validation loss 0.0897\n",
      "Epoch [76/200], Train loss: 0.0346, Validation loss 0.1013\n",
      "Epoch [77/200], Train loss: 0.1215, Validation loss 0.0915\n",
      "Epoch [78/200], Train loss: 0.0608, Validation loss 0.1128\n",
      "Epoch [79/200], Train loss: 0.0464, Validation loss 0.0850\n",
      "Epoch [80/200], Train loss: 0.0435, Validation loss 0.0980\n",
      "Epoch [81/200], Train loss: 0.0700, Validation loss 0.1062\n",
      "Epoch [82/200], Train loss: 0.0659, Validation loss 0.0967\n",
      "Epoch [83/200], Train loss: 0.0928, Validation loss 0.1031\n",
      "Epoch [84/200], Train loss: 0.0965, Validation loss 0.1272\n",
      "Epoch [85/200], Train loss: 0.0975, Validation loss 0.0931\n",
      "Epoch [86/200], Train loss: 0.0923, Validation loss 0.0850\n",
      "Epoch [87/200], Train loss: 0.1547, Validation loss 0.0985\n",
      "Epoch [88/200], Train loss: 0.1073, Validation loss 0.1071\n",
      "Training finished! after 88 epochs, Model saved to ./model-fold-5.pth\n",
      "Epoch [1/200], Train loss: 0.8236, Validation loss 0.7135\n",
      "Epoch [2/200], Train loss: 0.5420, Validation loss 0.5511\n",
      "Epoch [3/200], Train loss: 0.8214, Validation loss 0.5668\n",
      "Epoch [4/200], Train loss: 0.5267, Validation loss 0.6687\n",
      "Epoch [5/200], Train loss: 0.4237, Validation loss 0.5282\n",
      "Epoch [6/200], Train loss: 0.2102, Validation loss 0.2793\n",
      "Epoch [7/200], Train loss: 0.2139, Validation loss 0.3146\n",
      "Epoch [8/200], Train loss: 0.2800, Validation loss 0.3495\n",
      "Epoch [9/200], Train loss: 0.2579, Validation loss 0.2471\n",
      "Epoch [10/200], Train loss: 0.2513, Validation loss 0.4502\n",
      "Epoch [11/200], Train loss: 0.1512, Validation loss 0.2105\n",
      "Epoch [12/200], Train loss: 0.2074, Validation loss 0.2559\n",
      "Epoch [13/200], Train loss: 0.1873, Validation loss 0.1895\n",
      "Epoch [14/200], Train loss: 0.4256, Validation loss 0.3003\n",
      "Epoch [15/200], Train loss: 0.2589, Validation loss 0.2303\n",
      "Epoch [16/200], Train loss: 0.1264, Validation loss 0.1655\n",
      "Epoch [17/200], Train loss: 1.0083, Validation loss 0.3407\n",
      "Epoch [18/200], Train loss: 0.1152, Validation loss 0.1815\n",
      "Epoch [19/200], Train loss: 0.0828, Validation loss 0.1643\n",
      "Epoch [20/200], Train loss: 0.1028, Validation loss 0.1717\n",
      "Epoch [21/200], Train loss: 0.2541, Validation loss 0.1958\n",
      "Epoch [22/200], Train loss: 0.0969, Validation loss 0.1487\n",
      "Epoch [23/200], Train loss: 0.1373, Validation loss 0.1942\n",
      "Epoch [24/200], Train loss: 0.0861, Validation loss 0.1638\n",
      "Epoch [25/200], Train loss: 0.1724, Validation loss 0.2108\n",
      "Epoch [26/200], Train loss: 0.1836, Validation loss 0.2390\n",
      "Epoch [27/200], Train loss: 0.1013, Validation loss 0.1563\n",
      "Epoch [28/200], Train loss: 0.1609, Validation loss 0.1375\n",
      "Epoch [29/200], Train loss: 0.1894, Validation loss 0.1333\n",
      "Epoch [30/200], Train loss: 0.1197, Validation loss 0.1400\n",
      "Epoch [31/200], Train loss: 0.0702, Validation loss 0.1611\n",
      "Epoch [32/200], Train loss: 0.1086, Validation loss 0.1451\n",
      "Epoch [33/200], Train loss: 0.1552, Validation loss 0.1709\n",
      "Epoch [34/200], Train loss: 0.1928, Validation loss 0.1492\n",
      "Epoch [35/200], Train loss: 0.2339, Validation loss 0.1462\n",
      "Epoch [36/200], Train loss: 0.3610, Validation loss 0.1619\n",
      "Epoch [37/200], Train loss: 0.0648, Validation loss 0.1641\n",
      "Epoch [38/200], Train loss: 0.0955, Validation loss 0.1680\n",
      "Epoch [39/200], Train loss: 0.2005, Validation loss 0.2443\n",
      "Epoch [40/200], Train loss: 0.0764, Validation loss 0.1587\n",
      "Epoch [41/200], Train loss: 0.1428, Validation loss 0.1861\n",
      "Epoch [42/200], Train loss: 0.6040, Validation loss 0.1787\n",
      "Epoch [43/200], Train loss: 0.0672, Validation loss 0.1531\n",
      "Epoch [44/200], Train loss: 0.1208, Validation loss 0.1374\n",
      "Epoch [45/200], Train loss: 0.1311, Validation loss 0.1564\n",
      "Epoch [46/200], Train loss: 0.2454, Validation loss 0.1546\n",
      "Epoch [47/200], Train loss: 0.1880, Validation loss 0.1575\n",
      "Epoch [48/200], Train loss: 0.0836, Validation loss 0.1705\n",
      "Epoch [49/200], Train loss: 0.1572, Validation loss 0.1428\n",
      "Epoch [50/200], Train loss: 0.0629, Validation loss 0.1503\n",
      "Epoch [51/200], Train loss: 0.4705, Validation loss 0.1914\n",
      "Epoch [52/200], Train loss: 0.3577, Validation loss 0.2159\n",
      "Epoch [53/200], Train loss: 0.0930, Validation loss 0.2484\n",
      "Epoch [54/200], Train loss: 0.0756, Validation loss 0.1396\n",
      "Epoch [55/200], Train loss: 0.0508, Validation loss 0.1770\n",
      "Epoch [56/200], Train loss: 0.0960, Validation loss 0.1754\n",
      "Epoch [57/200], Train loss: 0.5213, Validation loss 0.1751\n",
      "Epoch [58/200], Train loss: 0.0946, Validation loss 0.2103\n",
      "Epoch [59/200], Train loss: 0.1406, Validation loss 0.1428\n",
      "Epoch [60/200], Train loss: 0.0730, Validation loss 0.1457\n",
      "Epoch [61/200], Train loss: 0.1237, Validation loss 0.1470\n",
      "Epoch [62/200], Train loss: 0.0901, Validation loss 0.1443\n",
      "Epoch [63/200], Train loss: 0.4246, Validation loss 0.1877\n",
      "Epoch [64/200], Train loss: 0.0913, Validation loss 0.1708\n",
      "Epoch [65/200], Train loss: 0.2962, Validation loss 0.2655\n",
      "Epoch [66/200], Train loss: 0.0913, Validation loss 0.1851\n",
      "Epoch [67/200], Train loss: 0.0978, Validation loss 0.1984\n",
      "Epoch [68/200], Train loss: 0.1043, Validation loss 0.1907\n",
      "Epoch [69/200], Train loss: 0.0664, Validation loss 0.1439\n",
      "Epoch [70/200], Train loss: 0.0948, Validation loss 0.1577\n",
      "Epoch [71/200], Train loss: 0.0857, Validation loss 0.1639\n",
      "Epoch [72/200], Train loss: 0.3228, Validation loss 0.1702\n",
      "Epoch [73/200], Train loss: 0.1064, Validation loss 0.1578\n",
      "Training finished! after 73 epochs, Model saved to ./model-fold-6.pth\n",
      "Epoch [1/200], Train loss: 0.5496, Validation loss 0.6120\n",
      "Epoch [2/200], Train loss: 0.6123, Validation loss 0.9025\n",
      "Epoch [3/200], Train loss: 0.4525, Validation loss 0.4537\n",
      "Epoch [4/200], Train loss: 0.3885, Validation loss 0.3565\n",
      "Epoch [5/200], Train loss: 0.3255, Validation loss 0.3477\n",
      "Epoch [6/200], Train loss: 0.2647, Validation loss 0.2881\n",
      "Epoch [7/200], Train loss: 0.2537, Validation loss 0.2732\n",
      "Epoch [8/200], Train loss: 0.2530, Validation loss 0.2470\n",
      "Epoch [9/200], Train loss: 0.1996, Validation loss 0.2194\n",
      "Epoch [10/200], Train loss: 0.3127, Validation loss 0.2380\n",
      "Epoch [11/200], Train loss: 0.1636, Validation loss 0.2135\n",
      "Epoch [12/200], Train loss: 0.1646, Validation loss 0.1818\n",
      "Epoch [13/200], Train loss: 0.3450, Validation loss 0.2024\n",
      "Epoch [14/200], Train loss: 0.1345, Validation loss 0.2551\n",
      "Epoch [15/200], Train loss: 0.1747, Validation loss 0.1560\n",
      "Epoch [16/200], Train loss: 0.1220, Validation loss 0.1552\n",
      "Epoch [17/200], Train loss: 0.4720, Validation loss 0.2475\n",
      "Epoch [18/200], Train loss: 0.1391, Validation loss 0.1371\n",
      "Epoch [19/200], Train loss: 0.5041, Validation loss 0.1512\n",
      "Epoch [20/200], Train loss: 0.2129, Validation loss 0.1324\n",
      "Epoch [21/200], Train loss: 0.1514, Validation loss 0.1232\n",
      "Epoch [22/200], Train loss: 0.1201, Validation loss 0.1390\n",
      "Epoch [23/200], Train loss: 0.0926, Validation loss 0.1233\n",
      "Epoch [24/200], Train loss: 0.1396, Validation loss 0.1177\n",
      "Epoch [25/200], Train loss: 0.1174, Validation loss 0.1118\n",
      "Epoch [26/200], Train loss: 0.0737, Validation loss 0.1705\n",
      "Epoch [27/200], Train loss: 0.1032, Validation loss 0.1079\n",
      "Epoch [28/200], Train loss: 0.0971, Validation loss 0.1872\n",
      "Epoch [29/200], Train loss: 0.1196, Validation loss 0.1741\n",
      "Epoch [30/200], Train loss: 0.0464, Validation loss 0.1237\n",
      "Epoch [31/200], Train loss: 0.1224, Validation loss 0.1513\n",
      "Epoch [32/200], Train loss: 0.0370, Validation loss 0.1564\n",
      "Epoch [33/200], Train loss: 0.0623, Validation loss 0.1119\n",
      "Epoch [34/200], Train loss: 0.3041, Validation loss 0.1489\n",
      "Epoch [35/200], Train loss: 0.0829, Validation loss 0.1673\n",
      "Epoch [36/200], Train loss: 0.0939, Validation loss 0.1151\n",
      "Epoch [37/200], Train loss: 0.0637, Validation loss 0.1227\n",
      "Epoch [38/200], Train loss: 0.0697, Validation loss 0.1118\n",
      "Epoch [39/200], Train loss: 0.2306, Validation loss 0.1441\n",
      "Epoch [40/200], Train loss: 0.0398, Validation loss 0.1887\n",
      "Epoch [41/200], Train loss: 0.0895, Validation loss 0.1383\n",
      "Epoch [42/200], Train loss: 0.1004, Validation loss 0.1087\n",
      "Epoch [43/200], Train loss: 0.1679, Validation loss 0.1156\n",
      "Epoch [44/200], Train loss: 0.0282, Validation loss 0.1167\n",
      "Epoch [45/200], Train loss: 0.5498, Validation loss 0.0996\n",
      "Epoch [46/200], Train loss: 0.0683, Validation loss 0.1813\n",
      "Epoch [47/200], Train loss: 0.0571, Validation loss 0.1194\n",
      "Epoch [48/200], Train loss: 0.4242, Validation loss 0.1246\n",
      "Epoch [49/200], Train loss: 0.0494, Validation loss 0.0976\n",
      "Epoch [50/200], Train loss: 0.1077, Validation loss 0.1172\n",
      "Epoch [51/200], Train loss: 0.0682, Validation loss 0.1303\n",
      "Epoch [52/200], Train loss: 0.0434, Validation loss 0.0998\n",
      "Epoch [53/200], Train loss: 0.0720, Validation loss 0.1084\n",
      "Epoch [54/200], Train loss: 0.0989, Validation loss 0.1163\n",
      "Epoch [55/200], Train loss: 0.2948, Validation loss 0.1281\n",
      "Epoch [56/200], Train loss: 0.0575, Validation loss 0.1324\n",
      "Epoch [57/200], Train loss: 0.1056, Validation loss 0.1429\n",
      "Epoch [58/200], Train loss: 0.2875, Validation loss 0.2081\n",
      "Epoch [59/200], Train loss: 0.0500, Validation loss 0.1157\n",
      "Epoch [60/200], Train loss: 0.1083, Validation loss 0.1257\n",
      "Epoch [61/200], Train loss: 0.0762, Validation loss 0.1153\n",
      "Epoch [62/200], Train loss: 0.0367, Validation loss 0.1564\n",
      "Epoch [63/200], Train loss: 0.0427, Validation loss 0.1422\n",
      "Epoch [64/200], Train loss: 0.1290, Validation loss 0.1406\n",
      "Epoch [65/200], Train loss: 0.2837, Validation loss 0.1674\n",
      "Epoch [66/200], Train loss: 0.1321, Validation loss 0.1467\n",
      "Epoch [67/200], Train loss: 0.6540, Validation loss 0.1707\n",
      "Epoch [68/200], Train loss: 0.1716, Validation loss 0.1618\n",
      "Epoch [69/200], Train loss: 0.1121, Validation loss 0.1454\n",
      "Epoch [70/200], Train loss: 0.1549, Validation loss 0.1456\n",
      "Epoch [71/200], Train loss: 0.0774, Validation loss 0.1436\n",
      "Training finished! after 71 epochs, Model saved to ./model-fold-7.pth\n",
      "Epoch [1/200], Train loss: 0.6988, Validation loss 0.7553\n",
      "Epoch [2/200], Train loss: 0.5210, Validation loss 0.5604\n",
      "Epoch [3/200], Train loss: 0.3677, Validation loss 0.4400\n",
      "Epoch [4/200], Train loss: 0.3841, Validation loss 0.3848\n",
      "Epoch [5/200], Train loss: 0.4104, Validation loss 0.4160\n",
      "Epoch [6/200], Train loss: 0.3249, Validation loss 0.3450\n",
      "Epoch [7/200], Train loss: 0.3084, Validation loss 0.2843\n",
      "Epoch [8/200], Train loss: 0.2037, Validation loss 0.2443\n",
      "Epoch [9/200], Train loss: 0.2044, Validation loss 0.2347\n",
      "Epoch [10/200], Train loss: 0.1784, Validation loss 0.2050\n",
      "Epoch [11/200], Train loss: 0.1508, Validation loss 0.1847\n",
      "Epoch [12/200], Train loss: 0.1498, Validation loss 0.1792\n",
      "Epoch [13/200], Train loss: 0.1927, Validation loss 0.1899\n",
      "Epoch [14/200], Train loss: 0.1955, Validation loss 0.1659\n",
      "Epoch [15/200], Train loss: 0.1534, Validation loss 0.1542\n",
      "Epoch [16/200], Train loss: 0.1524, Validation loss 0.1992\n",
      "Epoch [17/200], Train loss: 0.1272, Validation loss 0.1481\n",
      "Epoch [18/200], Train loss: 0.1356, Validation loss 0.1485\n",
      "Epoch [19/200], Train loss: 0.1396, Validation loss 0.1301\n",
      "Epoch [20/200], Train loss: 0.1654, Validation loss 0.1303\n",
      "Epoch [21/200], Train loss: 0.1222, Validation loss 0.1161\n",
      "Epoch [22/200], Train loss: 0.1174, Validation loss 0.1267\n",
      "Epoch [23/200], Train loss: 0.0767, Validation loss 0.1153\n",
      "Epoch [24/200], Train loss: 0.0528, Validation loss 0.1403\n",
      "Epoch [25/200], Train loss: 0.0950, Validation loss 0.2071\n",
      "Epoch [26/200], Train loss: 0.0869, Validation loss 0.1064\n",
      "Epoch [27/200], Train loss: 0.0972, Validation loss 0.1181\n",
      "Epoch [28/200], Train loss: 0.1779, Validation loss 0.1134\n",
      "Epoch [29/200], Train loss: 0.0968, Validation loss 0.1040\n",
      "Epoch [30/200], Train loss: 0.1111, Validation loss 0.1093\n",
      "Epoch [31/200], Train loss: 0.1303, Validation loss 0.1189\n",
      "Epoch [32/200], Train loss: 0.1521, Validation loss 0.1016\n",
      "Epoch [33/200], Train loss: 0.1389, Validation loss 0.2007\n",
      "Epoch [34/200], Train loss: 0.1087, Validation loss 0.1529\n",
      "Epoch [35/200], Train loss: 0.1059, Validation loss 0.1081\n",
      "Epoch [36/200], Train loss: 0.0662, Validation loss 0.1001\n",
      "Epoch [37/200], Train loss: 0.1912, Validation loss 0.3550\n",
      "Epoch [38/200], Train loss: 0.0742, Validation loss 0.1129\n",
      "Epoch [39/200], Train loss: 0.1383, Validation loss 0.1254\n",
      "Epoch [40/200], Train loss: 0.0801, Validation loss 0.1076\n",
      "Epoch [41/200], Train loss: 0.2099, Validation loss 0.1222\n",
      "Epoch [42/200], Train loss: 0.1127, Validation loss 0.0985\n",
      "Epoch [43/200], Train loss: 0.1269, Validation loss 0.1004\n",
      "Epoch [44/200], Train loss: 0.0931, Validation loss 0.0986\n",
      "Epoch [45/200], Train loss: 0.1493, Validation loss 0.1153\n",
      "Epoch [46/200], Train loss: 0.0480, Validation loss 0.0968\n",
      "Epoch [47/200], Train loss: 0.2255, Validation loss 0.1085\n",
      "Epoch [48/200], Train loss: 0.0658, Validation loss 0.0850\n",
      "Epoch [49/200], Train loss: 0.1003, Validation loss 0.0924\n",
      "Epoch [50/200], Train loss: 0.0622, Validation loss 0.0898\n",
      "Epoch [51/200], Train loss: 0.0746, Validation loss 0.0890\n",
      "Epoch [52/200], Train loss: 0.0821, Validation loss 0.0857\n",
      "Epoch [53/200], Train loss: 0.0938, Validation loss 0.0999\n",
      "Epoch [54/200], Train loss: 0.0861, Validation loss 0.1059\n",
      "Epoch [55/200], Train loss: 0.0404, Validation loss 0.1100\n",
      "Epoch [56/200], Train loss: 0.0865, Validation loss 0.0853\n",
      "Epoch [57/200], Train loss: 0.0533, Validation loss 0.0912\n",
      "Epoch [58/200], Train loss: 0.1086, Validation loss 0.0790\n",
      "Epoch [59/200], Train loss: 0.1185, Validation loss 0.0998\n",
      "Epoch [60/200], Train loss: 0.0575, Validation loss 0.1043\n",
      "Epoch [61/200], Train loss: 0.2140, Validation loss 0.1456\n",
      "Epoch [62/200], Train loss: 0.0388, Validation loss 0.0892\n",
      "Epoch [63/200], Train loss: 0.1228, Validation loss 0.1075\n",
      "Epoch [64/200], Train loss: 0.1302, Validation loss 0.0841\n",
      "Epoch [65/200], Train loss: 0.2380, Validation loss 0.1376\n",
      "Epoch [66/200], Train loss: 0.0917, Validation loss 0.0908\n",
      "Epoch [67/200], Train loss: 0.0738, Validation loss 0.0842\n",
      "Epoch [68/200], Train loss: 0.0611, Validation loss 0.0803\n",
      "Epoch [69/200], Train loss: 0.1012, Validation loss 0.0895\n",
      "Epoch [70/200], Train loss: 0.0919, Validation loss 0.0824\n",
      "Epoch [71/200], Train loss: 0.0876, Validation loss 0.0792\n",
      "Epoch [72/200], Train loss: 0.1558, Validation loss 0.1358\n",
      "Epoch [73/200], Train loss: 0.1392, Validation loss 0.1805\n",
      "Epoch [74/200], Train loss: 0.0732, Validation loss 0.1034\n",
      "Epoch [75/200], Train loss: 0.1386, Validation loss 0.1785\n",
      "Epoch [76/200], Train loss: 0.0965, Validation loss 0.1900\n",
      "Epoch [77/200], Train loss: 0.0857, Validation loss 0.1009\n",
      "Training finished! after 77 epochs, Model saved to ./model-fold-8.pth\n",
      "Epoch [1/200], Train loss: 0.9089, Validation loss 0.7265\n",
      "Epoch [2/200], Train loss: 0.8272, Validation loss 0.9105\n",
      "Epoch [3/200], Train loss: 0.5268, Validation loss 0.5519\n",
      "Epoch [4/200], Train loss: 0.3978, Validation loss 0.3979\n",
      "Epoch [5/200], Train loss: 0.3022, Validation loss 0.3454\n",
      "Epoch [6/200], Train loss: 0.3128, Validation loss 0.2955\n",
      "Epoch [7/200], Train loss: 0.2181, Validation loss 0.2603\n",
      "Epoch [8/200], Train loss: 0.2077, Validation loss 0.2346\n",
      "Epoch [9/200], Train loss: 0.2537, Validation loss 0.2344\n",
      "Epoch [10/200], Train loss: 0.1888, Validation loss 0.2533\n",
      "Epoch [11/200], Train loss: 0.3172, Validation loss 0.2080\n",
      "Epoch [12/200], Train loss: 0.1696, Validation loss 0.2014\n",
      "Epoch [13/200], Train loss: 0.1599, Validation loss 0.1745\n",
      "Epoch [14/200], Train loss: 0.2441, Validation loss 0.2769\n",
      "Epoch [15/200], Train loss: 0.2050, Validation loss 0.1825\n",
      "Epoch [16/200], Train loss: 0.1289, Validation loss 0.1578\n",
      "Epoch [17/200], Train loss: 0.1049, Validation loss 0.1540\n",
      "Epoch [18/200], Train loss: 0.0887, Validation loss 0.1452\n",
      "Epoch [19/200], Train loss: 0.1015, Validation loss 0.1741\n",
      "Epoch [20/200], Train loss: 0.2143, Validation loss 0.1348\n",
      "Epoch [21/200], Train loss: 0.1325, Validation loss 0.1225\n",
      "Epoch [22/200], Train loss: 0.1923, Validation loss 0.1252\n",
      "Epoch [23/200], Train loss: 0.1514, Validation loss 0.1325\n",
      "Epoch [24/200], Train loss: 0.1551, Validation loss 0.1558\n",
      "Epoch [25/200], Train loss: 0.1015, Validation loss 0.1260\n",
      "Epoch [26/200], Train loss: 0.0466, Validation loss 0.1525\n",
      "Epoch [27/200], Train loss: 0.1670, Validation loss 0.1337\n",
      "Epoch [28/200], Train loss: 0.3118, Validation loss 0.1776\n",
      "Epoch [29/200], Train loss: 0.1115, Validation loss 0.1249\n",
      "Epoch [30/200], Train loss: 0.1712, Validation loss 0.1110\n",
      "Epoch [31/200], Train loss: 0.1252, Validation loss 0.2362\n",
      "Epoch [32/200], Train loss: 0.0962, Validation loss 0.1194\n",
      "Epoch [33/200], Train loss: 0.0890, Validation loss 0.1157\n",
      "Epoch [34/200], Train loss: 0.2025, Validation loss 0.1109\n",
      "Epoch [35/200], Train loss: 0.0733, Validation loss 0.1119\n",
      "Epoch [36/200], Train loss: 0.2070, Validation loss 0.1046\n",
      "Epoch [37/200], Train loss: 0.1279, Validation loss 0.1301\n",
      "Epoch [38/200], Train loss: 0.0768, Validation loss 0.1063\n",
      "Epoch [39/200], Train loss: 0.0905, Validation loss 0.1009\n",
      "Epoch [40/200], Train loss: 0.1014, Validation loss 0.1194\n",
      "Epoch [41/200], Train loss: 0.2399, Validation loss 0.1305\n",
      "Epoch [42/200], Train loss: 0.1341, Validation loss 0.1473\n",
      "Epoch [43/200], Train loss: 0.1245, Validation loss 0.2170\n",
      "Epoch [44/200], Train loss: 0.1487, Validation loss 0.1598\n",
      "Epoch [45/200], Train loss: 0.1137, Validation loss 0.1489\n",
      "Epoch [46/200], Train loss: 0.2841, Validation loss 0.1236\n",
      "Epoch [47/200], Train loss: 0.4170, Validation loss 0.1159\n",
      "Epoch [48/200], Train loss: 0.1594, Validation loss 0.1087\n",
      "Epoch [49/200], Train loss: 0.0678, Validation loss 0.1083\n",
      "Epoch [50/200], Train loss: 0.1093, Validation loss 0.1191\n",
      "Epoch [51/200], Train loss: 0.1011, Validation loss 0.1139\n",
      "Epoch [52/200], Train loss: 0.0443, Validation loss 0.1037\n",
      "Epoch [53/200], Train loss: 0.0802, Validation loss 0.1204\n",
      "Epoch [54/200], Train loss: 0.0925, Validation loss 0.1079\n",
      "Epoch [55/200], Train loss: 0.0698, Validation loss 0.1083\n",
      "Epoch [56/200], Train loss: 0.1411, Validation loss 0.1080\n",
      "Epoch [57/200], Train loss: 0.1312, Validation loss 0.1138\n",
      "Epoch [58/200], Train loss: 0.1056, Validation loss 0.0887\n",
      "Epoch [59/200], Train loss: 0.0876, Validation loss 0.1110\n",
      "Epoch [60/200], Train loss: 0.0578, Validation loss 0.1022\n",
      "Epoch [61/200], Train loss: 0.0580, Validation loss 0.1039\n",
      "Epoch [62/200], Train loss: 0.0809, Validation loss 0.1114\n",
      "Epoch [63/200], Train loss: 0.1020, Validation loss 0.1391\n",
      "Epoch [64/200], Train loss: 0.0583, Validation loss 0.1024\n",
      "Epoch [65/200], Train loss: 0.1050, Validation loss 0.1094\n",
      "Epoch [66/200], Train loss: 0.1227, Validation loss 0.1329\n",
      "Epoch [67/200], Train loss: 0.0721, Validation loss 0.0949\n",
      "Epoch [68/200], Train loss: 0.0628, Validation loss 0.1134\n",
      "Epoch [69/200], Train loss: 0.1814, Validation loss 0.1208\n",
      "Epoch [70/200], Train loss: 0.1276, Validation loss 0.1028\n",
      "Epoch [71/200], Train loss: 0.0667, Validation loss 0.1030\n",
      "Epoch [72/200], Train loss: 0.0647, Validation loss 0.1120\n",
      "Epoch [73/200], Train loss: 0.0593, Validation loss 0.1030\n",
      "Epoch [74/200], Train loss: 0.0662, Validation loss 0.1070\n",
      "Epoch [75/200], Train loss: 0.0452, Validation loss 0.1171\n",
      "Epoch [76/200], Train loss: 0.0685, Validation loss 0.1096\n",
      "Epoch [77/200], Train loss: 0.0320, Validation loss 0.1061\n",
      "Training finished! after 77 epochs, Model saved to ./model-fold-9.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model_arch:model_type,fold_path,batch_size = 4,\n",
    "num_epochs = 200,\n",
    "learning_rate = 1e-4,\n",
    "decay = 1e-5,\n",
    "num_trials = 10,\n",
    "num_classes = 4,\n",
    "num_pages = 2,\n",
    "patch_size=224,resize=(1120,1344)):\n",
    "    if not os.path.exists(fold_path):\n",
    "        os.makedirs(fold_path)\n",
    "\n",
    "    losses = [[[] for _ in range(num_trials)],[[] for _ in range(num_trials)]] # training , validation\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(num_trials):\n",
    "\n",
    "        #Change the enum to U_Net to train a U-net model.\n",
    "        model = load_model(model_arch,num_classes=num_classes)\n",
    "        model.to(device)\n",
    "\n",
    "        # Get data\n",
    "        train_subset,val_subset,_ = generate_set(patch_size=patch_size,resize=resize,filepath=\"D:/Skola/Avancerad AI/\",indices=i*num_pages,num_pages=num_pages)\n",
    "\n",
    "        # Done to created weights for the loss function, it might be better to do this on patches to include the true distribution based on dynamically added crops\n",
    "        class_counts = np.zeros(num_classes)\n",
    "        for page in train_subset: #Sum up the number of pixels for each class for each page\n",
    "            for class_id in range(num_classes):\n",
    "                class_counts[class_id] += np.sum(page.gt == class_id)\n",
    "\n",
    "        total_pixels = np.sum(class_counts) #Count the total of pixels\n",
    "        frequencies = class_counts / total_pixels\n",
    "        weights = torch.tensor(np.sqrt(1.0 / frequencies).astype('float32')) # calculate weights\n",
    "        weights\n",
    "        weights = weights.to(device)\n",
    "        # Loss, optimizer, stopper\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=decay)\n",
    "        earlystopper= EarlyStopping(20,0, f'./{fold_path}/model-small-fold-{i}.pth')\n",
    "\n",
    "\n",
    "        #Prepare data\n",
    "        train_dataset = PatchesDataset(train_subset,patches_transforms)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = PatchesDataset(val_subset,patches_transforms)\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            train_dataset.random_patch_generator(10)\n",
    "            val_dataset.random_patch_generator(10)\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)['out']\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            losses[0][i].append(train_loss)\n",
    "\n",
    "            #Validation loop\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)['out']\n",
    "                    loss = criterion(outputs,labels)\n",
    "                    running_loss += loss.item() * labels.size(0)\n",
    "            val_loss = running_loss/len(val_loader.dataset)\n",
    "            losses[1][i].append(val_loss)\n",
    "            if epoch > 50:\n",
    "                earlystopper(val_loss=val_loss,model=model,verbose=False)\n",
    "            if earlystopper.early_stop == True:\n",
    "                    break\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train loss: {loss.item():.4f}, Validation loss {val_loss:.4f}')\n",
    "\n",
    "        print(f'Training finished! after {epoch} epochs, Model saved to ./model-fold-{i}.pth')\n",
    "    return losses\n",
    "\n",
    "losses = train_model(model_type.L_U_Net,'L-unet-folds')\n",
    "losses2 = train_model(model_type.U_Net,'unet-folds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'hello' already exists.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"hello\"\n",
    "\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
